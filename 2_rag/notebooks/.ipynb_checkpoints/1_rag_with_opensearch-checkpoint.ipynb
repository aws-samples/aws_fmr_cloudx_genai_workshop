{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain & Amazon OpenSearch\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Previously, we used the Anthropic Claude model in Amazon Bedrock to demonstrate a basic Question Answering (QA) system, and learned the value of grounding a model with additional context before generating a response. In the previous notebook, we had to manually provide the model with relevant data and context ourselves. However, this approach is not fit for enterprise-level QA systems where there could be hundreds of thousands of large documents.\n",
    "\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "We can improve upon this process by implementing an architecture called retrieval augmented generation (RAG). RAG retrieves data from outside the LLM's training data sources and augments the prompts by adding the relevant retrieved data as context. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, without needing to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
    "\n",
    "## Solution\n",
    "\n",
    "In this notebook, we augment LLM responses to user queries by implementing RAG using context from external documents. First, we process documents and store these into a vector store. Next, we search the vector store using the user's question, and return relevant data as external context to the LLM. Finally, the LLM generates an answer to the user's question based on the new context provided.\n",
    "\n",
    "We will walk through implementing the following two patterns: Question Answering (QA) and Conversational AI with conversation memory. \n",
    "\n",
    "Let’s break down the solution a little further. \n",
    "\n",
    "### Prepare documents for search\n",
    "![Documents](../images/rag1.png)\n",
    "\n",
    "First, the documents must be processed and then indexed in a vector store.\n",
    "- Load the documents from our directory\n",
    "- Process the documents by splitting them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using an embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "\n",
    "### Respond to the user’s question\n",
    "![Question](../images/rag2.png)\n",
    "\n",
    "Once the vector store is indexed with documents and embeddings, we can search for text relevant to the question being asked. The relevant chunks are sent to the model as additional context, where the model will then generate the answer.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved\n",
    "\n",
    "![Question](../images/rag3.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed and to connect to Amazon Bedrock.\n",
    "\n",
    "⚠️ For more details on how the setup works and **whether you might need to make any changes**, refer to the [Amazon Bedrock boto3 setup notebook](../../02_prompt_engineering/1_setup.ipynb) notebook.\n",
    "\n",
    "In this notebook, we'll also need some extra dependencies:\n",
    "\n",
    "- [OpenSearch Python Client](https://pypi.org/project/opensearch-py/) to store vector embeddings\n",
    "- [PyPDF](https://pypi.org/project/pypdf/) for handling PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3>=1.28.79 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (1.34.101)\n",
      "Requirement already satisfied: botocore>=1.31.79 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.34.101)\n",
      "Requirement already satisfied: Pillow==9.4.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (9.4.0)\n",
      "Requirement already satisfied: streamlit==1.27.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (1.27.0)\n",
      "Requirement already satisfied: streamlit-chat==0.1.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: langchain==0.1.14 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (0.1.14)\n",
      "Requirement already satisfied: faiss-cpu<2,>=1.7 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.8.0)\n",
      "Requirement already satisfied: unstructured==0.10.16 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (0.10.16)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.21 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (2.0.21)\n",
      "Requirement already satisfied: llama-index==0.9.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: xmltodict==0.13.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (0.13.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (13.7.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (5.3.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (6.11.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=6.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (15.0.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3,>=2.18 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (5.2)\n",
      "Requirement already satisfied: validators<1,>=0.2 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (6.4)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.0->-r ../requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (0.1.56)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14->-r ../requirements.txt (line 6)) (2.7.1)\n",
      "Requirement already satisfied: chardet in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (5.2.0)\n",
      "Requirement already satisfied: filetype in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (0.4.27)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (5.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (4.12.3)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (2.11.1)\n",
      "Requirement already satisfied: python-iso639 in /opt/conda/lib/python3.10/site-packages (from unstructured==0.10.16->-r ../requirements.txt (line 8)) (2024.4.27)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy==2.0.21->-r ../requirements.txt (line 9)) (3.0.3)\n",
      "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (0.5.2)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.6.0)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.27.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.26.18)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.28.79->-r ../requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.28.79->-r ../requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->-r ../requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->-r ../requirements.txt (line 12)) (2.17.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14->-r ../requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14->-r ../requirements.txt (line 6)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14->-r ../requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14->-r ../requirements.txt (line 6)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14->-r ../requirements.txt (line 6)) (1.9.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (4.21.1)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured==0.10.16->-r ../requirements.txt (line 8)) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14->-r ../requirements.txt (line 6)) (3.21.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.27.0->-r ../requirements.txt (line 4)) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit==1.27.0->-r ../requirements.txt (line 4)) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.14->-r ../requirements.txt (line 6)) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14->-r ../requirements.txt (line 6)) (3.10.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->-r ../requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.10.16->-r ../requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.10.16->-r ../requirements.txt (line 8)) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.10.16->-r ../requirements.txt (line 8)) (4.66.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.9.0->-r ../requirements.txt (line 10)) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index==0.9.0->-r ../requirements.txt (line 10)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index==0.9.0->-r ../requirements.txt (line 10)) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index==0.9.0->-r ../requirements.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14->-r ../requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14->-r ../requirements.txt (line 6)) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.3->streamlit==1.27.0->-r ../requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.18->streamlit==1.27.0->-r ../requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.9.0->-r ../requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.27.0->-r ../requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.0->-r ../requirements.txt (line 4)) (0.18.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py==2.3.1\n",
      "  Using cached opensearch_py-2.3.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting apache-beam\n",
      "  Using cached apache_beam-2.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (13.7.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.26.18)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2024.2.2)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam)\n",
      "  Using cached crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: orjson<4,>=3.9.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (3.10.3)\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam)\n",
      "  Using cached dill-0.3.1.1.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (2.2.1)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam)\n",
      "  Using cached fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam)\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<2,>=1.33.1 (from apache-beam)\n",
      "  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam)\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m802.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httplib2<0.23.0,>=0.8 (from apache-beam)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (4.21.1)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam)\n",
      "  Downloading jsonpickle-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.14.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (1.26.4)\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam)\n",
      "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (23.2)\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam)\n",
      "  Downloading pymongo-4.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache-beam)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (4.25.3)\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (2024.1)\n",
      "Collecting redis<6,>=5.0.0 (from apache-beam)\n",
      "  Downloading redis-5.0.4-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (2023.12.25)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (4.11.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (0.22.0)\n",
      "Collecting pyarrow<15.0.0,>=3.0.0 (from apache-beam)\n",
      "  Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in /opt/conda/lib/python3.10/site-packages (from apache-beam) (0.6)\n",
      "Collecting js2py<1,>=0.74 (from apache-beam)\n",
      "  Downloading Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich) (2.17.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache-beam) (3.1.2)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam) (5.2)\n",
      "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam)\n",
      "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.6)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading multiprocess-0.70.13-py310-none-any.whl.metadata (6.8 kB)\n",
      "  Downloading multiprocess-0.70.12.2-py39-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading multiprocess-0.70.12.1.zip (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading multiprocess-0.70.12-py39-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading multiprocess-0.70.11.1-py39-none-any.whl.metadata (5.1 kB)\n",
      "INFO: pip is still looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading multiprocess-0.70.11-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Downloading multiprocess-0.70.10.zip (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading multiprocess-0.70.9.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Downloading opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading apache_beam-2.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-3.0.4-py3-none-any.whl (39 kB)\n",
      "Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m628.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Downloading pymongo-4.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.0/670.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading redis-5.0.4-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: crcmod, dill, hdfs, multiprocess, pyjsparser, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=23534 sha256=4353e869f3c4e17242fc3e08bc4802fa378a254dcbdbe0f75a49ca592e4532c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78541 sha256=4c1053558aac974c726b31b2c6e5e15f50658ccf2c75c8abd93408a78fc53afb\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=49f036b41b6662d2593de5674c0baaa992fd62b9177cb4737cc41f7e4a0f0a06\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
      "  Building wheel for multiprocess (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multiprocess: filename=multiprocess-0.70.9-py3-none-any.whl size=70930 sha256=ddf6abdcd3f11ce381b4e3c522c11de0c91a10e3f3894c6aa5a01bb12cbab546\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/b9/ff/72cd56f34f0b3edde101afa3bf54c1ba85b771d51e7eaa7b03\n",
      "  Building wheel for pyjsparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=04e575c5c25af6e1eb2746247ea613d7a7fc3dfd18983bcfc16c814ad5ba54b2\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=627e211710aec90a7563d94f82b7425cb0a38c902cc8f6c2f7ee7a4ed582f982\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built crcmod dill hdfs multiprocess pyjsparser docopt\n",
      "Installing collected packages: pyjsparser, docopt, crcmod, redis, pydot, pyarrow, proto-plus, objsize, jsonpickle, js2py, httplib2, grpcio, fasteners, fastavro, dnspython, dill, pymongo, opensearch-py, multiprocess, hdfs, apache-beam\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 15.0.2\n",
      "    Uninstalling pyarrow-15.0.2:\n",
      "      Successfully uninstalled pyarrow-15.0.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed apache-beam-2.56.0 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 grpcio-1.63.0 hdfs-2.7.3 httplib2-0.22.0 js2py-0.74 jsonpickle-3.0.4 multiprocess-0.70.9 objsize-0.7.0 opensearch-py-2.3.1 proto-plus-1.23.0 pyarrow-14.0.2 pydot-1.4.2 pyjsparser-2.7.1 pymongo-4.7.2 redis-5.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opensearch-py==2.3.1 \\\n",
    "    apache-beam \\\n",
    "    datasets \\\n",
    "    tiktoken \\\n",
    "    rich "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from rich import print as rprint\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "module_path = \"../../\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LangChain\n",
    "\n",
    "LangChain provides convenient integrations with Amazon Bedrock and other services like vector stores and retrievers. We begin with instantiating the large language model (LLM) and the embeddings model. We are using Anthropic Claude models for text generation and Amazon Titan Embeddings G1 - Text for text embedding.\n",
    "\n",
    "Note: Amazon Bedrock offers a choice of high-performing foundation models (FMs). You can replace the value for `model_id` with one of the available [model IDs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) as follows. Some models have different requirements for inputs such as prompt format. As of this writing, all models are supported in the US West (Oregon, us-west-2) Region. If you are using another AWS Region, check the latest [model support by AWS Region](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html).\n",
    "\n",
    "```python\n",
    "llm = BedrockChat(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\", ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import ChatBedrock`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.load.dump import dumps\n",
    "\n",
    "# Instantiate the LLM\n",
    "\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=model_id,\n",
    "    model_kwargs={\"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Instantiate the Amazon Titan Embeddings G1 - Text embeddings model\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\" # change this model ID to use another embeddings model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase Introduction - Model Risk and Model Governance Assistant\n",
    "In this notebook we will learn the application of RAG through a practical example. The use case we will be working on is a Model Risk and Model Governance Assistant. This assistant will help users understand the risks associated with deploying machine learning models in production. The assistant will provide information on the following topics:\n",
    "- Model Risk Management\n",
    "- Model Governance\n",
    "- Regulatory Compliance\n",
    "- Model Monitoring\n",
    "- Model Validation\n",
    "- And more\n",
    "\n",
    "We will use some publicly available regulatory guideline documents to serve as the source for our RAG solution. You can vew the documents in the `../data/model_risk` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will load the documents with the help of [PyPDF in LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf).\n",
    "\n",
    "We will utilize a few different techniques when loading the documents that will help improve the retrieval quality.\n",
    "\n",
    "#### Outline based splitting\n",
    "By default LangChain's `PyPDFLoader` will break each document up into pages. We could then potentially use a chunking strategy such as `RecursiveCharacterTextSplitter` to further break down the pages into smaller chunks. \n",
    "However, this could lead to suboptimal results if the most relevant information we are looking for is split across multiple pages. Instead, we will split the documents into sections based on the documents own table of contents. The implementation for this approach is provided in the rag_utils.outline_parser module [(source)](./rag_utils/outline_parser.py).\n",
    "Note that this approach only works on PDFs that contain a table of contents.\n",
    "\n",
    "\n",
    "#### Parent Document Retriever\n",
    "After we've loaded the document as individual sections, we will further split these sections by paragraphs using the [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/). These are the chunks that will be used for embeddings, however during retrieval we'll utilize the [ParentDocumentRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever/) to retrieve the entire section that the chunk belongs to. This is done to ensure that the context provided to the model is as complete as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from utils.outline_parser import PyPDFOutlineParser\n",
    "\n",
    "docs_path = Path(\"../data/model_risk\")\n",
    "doc_files = list(docs_path.glob(\"*.pdf\"))\n",
    "\n",
    "doc_files\n",
    "\n",
    "section_chunks = []\n",
    "\n",
    "for doc_path in doc_files:\n",
    "    loader = PyPDFLoader(file_path=doc_path.as_posix())\n",
    "    loader.parser = PyPDFOutlineParser()\n",
    "    sections = loader.load()\n",
    "    for sec in sections:\n",
    "        sec.metadata.update({\"file\": doc_path.name})\n",
    "    \n",
    "    section_chunks += sections\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section chunk now contains a the contents and metadata associated with that section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Page 1 SR Letter  11-7 \\nAttachment \\nBoard of Governors of the Federal Reserve System \\nOffice </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the Comptroller of the Currency \\nApril 4, 2011 \\nSUPERVISORY GUIDANCE ON \\nMODEL RISK MANAGEMENT \\nCONTENTS </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\nI. Introduction, page 1 \\nII. Purpose and Scope, page 2 \\nIII. Overview of Model Risk Management, page 3 \\nIV. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model Development, Implementation, and Use, page 5 \\nV. Model Validation, page 9 \\nVI. Governance, Policies, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Controls, page 16 \\nVII. Conclusion, page 21 \\nI. INTRODUCTION \\nBanks rely heavily on quantitative analysis and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models in most aspects of financial \\ndecision making.  [Footnote 1 \\n- Unless otherwise indicated, banks refers to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">national banks and all other institutions for which the Office \\nof the Comptroller of the Currency is the primary </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supervisor, and to bank holding companies, state member \\nbanks, and all other institutions for which the Federal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Reserve Board is the primary supervisor. End of Footnote 1.] They routinely use models for a broad range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">activities, including \\nunderwriting credits; valuing exposures, instruments, and positions; measuring risk; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\nmanaging and safeguarding client assets; determining capital and reserve adequacy; and \\nmany other activities. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">In recent years, banks have applied models to more complex \\nproducts and with more ambitious scope, such as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enterprise-wide risk measurement, \\nwhile the markets in which they are used have also broadened and changed. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Changes in \\nregulation have spurred some of the recent developments, particularly the U.S. regulatory \\ncapital </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rules for market, credit, and operational risk based on the framework developed \\nby the Basel Committee on Banking</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Supervision. Even apart from these regulatory \\nconsiderations, however, banks have been increasing the use of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data-driven, quantitative \\ndecision-making tools for a number of years. \\nThe expanding use of models in all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aspects of banking reflects the extent to which models \\ncan improve business decisions, but models also come with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">costs. There is the direct cost \\nof devoting resources to develop and implement models properly. There are also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the \\npotential indirect costs of relying on models, such as the possible adverse consequences \\n(including </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">financial loss) of decisions based on models that are incorrect or misused. \\nThose consequences should be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">addressed by active management of model risk. [Page Break] \\n'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'section_title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Supervisory Guidance on Model Risk Management'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page_index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'parent_section'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'sr1107a1.pdf'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Page 1 SR Letter  11-7 \\nAttachment \\nBoard of Governors of the Federal Reserve System \\nOffice \u001b[0m\n",
       "\u001b[32mof the Comptroller of the Currency \\nApril 4, 2011 \\nSUPERVISORY GUIDANCE ON \\nMODEL RISK MANAGEMENT \\nCONTENTS \u001b[0m\n",
       "\u001b[32m\\nI. Introduction, page 1 \\nII. Purpose and Scope, page 2 \\nIII. Overview of Model Risk Management, page 3 \\nIV. \u001b[0m\n",
       "\u001b[32mModel Development, Implementation, and Use, page 5 \\nV. Model Validation, page 9 \\nVI. Governance, Policies, and \u001b[0m\n",
       "\u001b[32mControls, page 16 \\nVII. Conclusion, page 21 \\nI. INTRODUCTION \\nBanks rely heavily on quantitative analysis and \u001b[0m\n",
       "\u001b[32mmodels in most aspects of financial \\ndecision making.  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mFootnote 1 \\n- Unless otherwise indicated, banks refers to\u001b[0m\n",
       "\u001b[32mnational banks and all other institutions for which the Office \\nof the Comptroller of the Currency is the primary \u001b[0m\n",
       "\u001b[32msupervisor, and to bank holding companies, state member \\nbanks, and all other institutions for which the Federal \u001b[0m\n",
       "\u001b[32mReserve Board is the primary supervisor. End of Footnote 1.\u001b[0m\u001b[32m]\u001b[0m\u001b[32m They routinely use models for a broad range of \u001b[0m\n",
       "\u001b[32mactivities, including \\nunderwriting credits; valuing exposures, instruments, and positions; measuring risk; \u001b[0m\n",
       "\u001b[32m\\nmanaging and safeguarding client assets; determining capital and reserve adequacy; and \\nmany other activities. \u001b[0m\n",
       "\u001b[32mIn recent years, banks have applied models to more complex \\nproducts and with more ambitious scope, such as \u001b[0m\n",
       "\u001b[32menterprise-wide risk measurement, \\nwhile the markets in which they are used have also broadened and changed. \u001b[0m\n",
       "\u001b[32mChanges in \\nregulation have spurred some of the recent developments, particularly the U.S. regulatory \\ncapital \u001b[0m\n",
       "\u001b[32mrules for market, credit, and operational risk based on the framework developed \\nby the Basel Committee on Banking\u001b[0m\n",
       "\u001b[32mSupervision. Even apart from these regulatory \\nconsiderations, however, banks have been increasing the use of \u001b[0m\n",
       "\u001b[32mdata-driven, quantitative \\ndecision-making tools for a number of years. \\nThe expanding use of models in all \u001b[0m\n",
       "\u001b[32maspects of banking reflects the extent to which models \\ncan improve business decisions, but models also come with \u001b[0m\n",
       "\u001b[32mcosts. There is the direct cost \\nof devoting resources to develop and implement models properly. There are also \u001b[0m\n",
       "\u001b[32mthe \\npotential indirect costs of relying on models, such as the possible adverse consequences \\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mincluding \u001b[0m\n",
       "\u001b[32mfinancial loss\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of decisions based on models that are incorrect or misused. \\nThose consequences should be \u001b[0m\n",
       "\u001b[32maddressed by active management of model risk. \u001b[0m\u001b[32m[\u001b[0m\u001b[32mPage Break\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\n'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'section_title'\u001b[0m: \u001b[32m'Supervisory Guidance on Model Risk Management'\u001b[0m,\n",
       "        \u001b[32m'page_label'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
       "        \u001b[32m'page_index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "        \u001b[32m'parent_section'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'file'\u001b[0m: \u001b[32m'sr1107a1.pdf'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(section_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test out our embedding model on a single section to see what an embedding looks like below. These embeddings could be generated for the entire corpus of documents and stored in a vector store for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Embedding model Id : amazon.titan-embed-text-v1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Embedding model Id : amazon.titan-embed-text-v1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample embedding of a document chunk: \n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.111816406</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08544922</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.19433594</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6796875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.359375</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.087890625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01965332</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00032424927</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6015625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.28320312</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sample embedding of a document chunk: \n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m0.111816406\u001b[0m,\n",
       "    \u001b[1;36m-0.08544922\u001b[0m,\n",
       "    \u001b[1;36m-0.19433594\u001b[0m,\n",
       "    \u001b[1;36m-0.6796875\u001b[0m,\n",
       "    \u001b[1;36m0.359375\u001b[0m,\n",
       "    \u001b[1;36m-0.087890625\u001b[0m,\n",
       "    \u001b[1;36m0.01965332\u001b[0m,\n",
       "    \u001b[1;36m0.00032424927\u001b[0m,\n",
       "    \u001b[1;36m0.6015625\u001b[0m,\n",
       "    \u001b[1;36m-0.28320312\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size of the embedding:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size of the embedding:  \u001b[1;36m1536\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    sample_embedding = bedrock_embeddings.embed_query(section_chunks[0].page_content)\n",
    "    modelId = bedrock_embeddings.model_id\n",
    "    rprint(\"Embedding model Id :\", modelId)\n",
    "    rprint(\"Sample embedding of a document chunk: \", sample_embedding[:10])\n",
    "    rprint(\"Size of the embedding: \", len(sample_embedding))\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "        \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html\\\n",
    "        \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\\n",
    "        \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "              \\x1b[0m\")\n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the vector store\n",
    "In this workshop we will use the ***vector engine for Amazon OpenSearch Serverless.***\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application — without impacting data ingestion.\n",
    "\n",
    "The code block below will provision an OpenSearch Serverless collection and create the vector store that we will use to store the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a security policy for AOSS collection..\n",
      "Creating a network policy for AOSS collection..\n",
      "Creating an AOSS collection..\n",
      "Waiting for an AOSS collection to be created..\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from rag_utils.oss_utils import create_oss_resources, delete_oss_resources, get_aws_auth\n",
    "config_file = \"oss_config.json\"\n",
    "\n",
    "host, config = create_oss_resources(config_file, replace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to ingest the documents into the vector store. This can be done easily using the [LangChain OpenSearch integration](https://python.langchain.com/docs/integrations/vectorstores/opensearch) which takes in the embeddings model and the documents to create the entire vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "auth = get_aws_auth()\n",
    "\n",
    "\n",
    "vector_db = OpenSearchVectorSearch(\n",
    "    index_name=config[\"index_name\"],\n",
    "    embedding_function=bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 7200,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    engine=\"faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the `ParentDocumentRetriever` combining an OpenSearch based vector store and key-value based `InMemoryStore`. The vector store will be used to find section segments that were generated using through splitting with the `RecursiveCharacterSplitter`. Each section segment will contain a key reference to the full section document. The key reference will be used to retrieve the entire section text. Note that the `InMemoryStore` is essentially a python dictionary, in production you would want to use a persistent store such as [DynamoDB](https://aws.amazon.com/dynamodb/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\", \"\\n\\n\"], chunk_size=2000, chunk_overlap=250\n",
    ")\n",
    "\n",
    "in_memory_store_file = config[\"index_name\"] + \"_store.pkl\"\n",
    "\n",
    "# if we previously ingested the docs we can reuse the existing index\n",
    "if Path(in_memory_store_file).exists():\n",
    "    store = pickle.load(open(in_memory_store_file, \"rb\"))\n",
    "    \n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vector_db,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "    )\n",
    "\n",
    "# ingest the document into the index\n",
    "else:\n",
    "    store = InMemoryStore()\n",
    "    \n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vector_db,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "    )\n",
    "    \n",
    "    retriever.add_documents(section_chunks, ids=None)\n",
    "    pickle.dump(store, open(in_memory_store_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Searching the vector store\n",
    "Before we get into the parent document retrieval, let's first explore the various ways that we can query the vector store exclusively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Semantic search methods\n",
    "[Semantic search](https://opensearch.org/docs/latest/search-plugins/semantic-search/) considers the context and intent of a query. In OpenSearch, semantic search is facilitated by neural search with text embedding models. Semantic search creates a dense vector (a list of floats) and ingests data into a k-NN index.\n",
    "\n",
    "Short for k-nearest neighbors, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors.  To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. We will explore some of the distance functions later in this lab.\n",
    "\n",
    "The k-NN plugin supports three different methods for obtaining the k-nearest neighbors from an index of vectors:\n",
    "- Approximate k-NN\n",
    "- Script Score k-NN\n",
    "- Painless extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate k-NN search\n",
    "Standard k-NN search methods compute similarity using a brute-force approach that measures the nearest distance between a query and a number of points, which produces exact results. This works well in many applications. However, in the case of extremely large datasets with high dimensionality, this creates a scaling problem that reduces the efficiency of the search. [Approximate k-NN search](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/) methods can overcome this by employing tools that restructure indexes more efficiently and reduce the dimensionality of searchable vectors. Using this approach requires a sacrifice in accuracy but increases search processing speeds appreciably.\n",
    "\n",
    "Of the three search methods the k-NN plugin provides, this method offers the best search scalability for large datasets. This approach is the preferred method when a dataset reaches hundreds of thousands of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search query\n",
    "query = \"What can be considered a model?\"\n",
    "\n",
    "# Search for the 3 most relevant documents\n",
    "results = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "# sometimes it takes a few seconds for the vector store to be ready\n",
    "while len(results) == 0:\n",
    "    print(\"Vector store not yet ready\")\n",
    "    results = vector_db.similarity_search(query, k=3)\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact k-NN with scoring script\n",
    "The k-NN plugin implements the OpenSearch [score script](https://opensearch.org/docs/latest/search-plugins/knn/knn-score-script/) plugin that you can use to find the exact k-nearest neighbors to a given query point.\n",
    "\n",
    "Because the score script approach executes a brute force search, it doesn’t scale as well as the approximate approach. This approach is preferred for searches over smaller bodies of documents or when a pre-filter is needed. Using this approach on large indexes may lead to high latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = vector_db.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    is_appx_search=False,\n",
    "    search_type=\"script_scoring\"\n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Painless Scripting extensions (exact)\n",
    "Currently, the vector engine for Amazon OpenSearch serverless supports the approximate k-NN search and scoring script search methods. Below is an example of the [painless scripting](https://opensearch.org/docs/latest/search-plugins/knn/painless-functions/) search method on an Amazon OpenSearch service for your reference.\n",
    "\n",
    "Similar to the k-NN Script Score, you can use this method to perform a brute force, exact k-NN search across an index. This approach has slightly slower query performance compared to the k-NN scoring script. If your use case requires more customization over the final score, you should use this approach over k-NN scoring script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "results = docsearch.similarity_search(\n",
    "    query, \n",
    "    is_appx_search=False,\n",
    "    search_type=\"painless_scripting\"\n",
    ")\n",
    "\n",
    "print(dumps(results, pretty=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact k-NN search with filters\n",
    "You can apply [k-NN search with filters](https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/) with either the  scoring script or painless extension search methods. Filters can greatly reduce the number of vectors to be searched. Using the k-NN score script, you can apply a filter on an index before executing the nearest neighbor search (sometimes referred to as a pre-filter search). This is useful for dynamic search cases where the index body may vary based on other conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the acceptable model evaluation techniques?\"\n",
    "\n",
    "# filter on a specific document\n",
    "pre_filter = {\"bool\": {\"filter\": {\"match\": {\"metadata.file\": \"sr1107a1.pdf\"}}}}\n",
    "\n",
    "# Pre-filter results\n",
    "results = vector_db.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    is_appx_search=False,\n",
    "    search_type=\"script_scoring\",\n",
    "    pre_filter=pre_filter   \n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces - similarity or distance measures\n",
    "\n",
    "Approximate Search through OpenSearch supports the following similarity or distance measures:\n",
    "\n",
    "**Cosine similarity** – The cosine of the angle between two vectors in a vector space.\n",
    "\n",
    "**Euclidean distance** – The straight-line distance between points.\n",
    "\n",
    "**L1 (Manhattan) distance** – The sum of the differences of all of the vector components. L1 distance measures how many orthogonal city blocks you need to traverse from point A to point B.\n",
    "\n",
    "**L-infinity (chessboard) distance** – The number of moves a King would make on an n-dimensional chessboard. It’s different than Euclidean distance on the diagonals—a diagonal step on a 2-dimensional chessboard is 1.41 Euclidean units away, but 2 L-infinity units away.\n",
    "\n",
    "**Inner product** – The product of the magnitudes of two vectors and the cosine of the angle between them. Usually used for natural language processing (NLP) vector similarity.\n",
    "\n",
    "We can specify the distance measure in the `space_type` parameter when we load our documents as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\", # Options are: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Engines and algorithms\n",
    "The Approximate k-NN search methods leveraged by OpenSearch use approximate nearest neighbor (ANN) algorithms from the [NMSLIB](https://github.com/nmslib/nmslib), [FAISS](https://github.com/facebookresearch/faiss), and [Lucene](https://lucene.apache.org/) libraries to power k-NN search.\n",
    "\n",
    "The engine details are as follows:\n",
    "\n",
    "- Non-Metric Space Library (NMSLIB) – NMSLIB implements the HNSW ANN algorithm\n",
    "- Facebook AI Similarity Search (FAISS) – FAISS implements both HNSW and IVF ANN algorithms\n",
    "- Lucene – Lucene implements the HNSW algorithm\n",
    "\n",
    "Each of the three engines used for approximate k-NN search has its own attributes that make one more sensible to use than the others in a given situation. In general, NMSLIB and FAISS should be selected for large-scale use cases. Lucene is a good option for smaller deployments, but offers benefits like smart filtering where the optimal filtering strategy—pre-filtering, post-filtering, or exact k-NN—is automatically applied depending on the situation.\n",
    "\n",
    "We can specify the engine as shown below.\n",
    "\n",
    "Note: As of this writing, Amazon OpenSearch Serverless vector search collections don't support the Apache Lucene ANN engine. Vector search collections only support the HNSW algorithm with FAISS and do not support IVF and IVFQ. Please check the updated [limitations](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html#serverless-vector-limitations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\", # Options are: “nmslib”, “faiss”, “lucene”; default: “nmslib”\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For HNSW, we can tune the m, ef_construction, and ef_search parameters to achieve our desired trade-off:**\n",
    "\n",
    "**m** – Controls the maximum number of edges a node in a graph can have. Because each node has to store all of its edges, increasing this value will increase the memory footprint, but also increase the connectivity of the graph, which will improve recall.\n",
    "\n",
    "**ef_construction** – Controls the size of the candidate queue for edges when adding a node to the graph. Increasing this value will increase the number of candidates to consider, which will increase the index latency. However, because more candidates will be considered, the quality of the graph will be better, leading to better recall during search.\n",
    "\n",
    "**ef_search** – Similar to ef_construction, it controls the size of the candidate queue for graph traversal during search. Increasing this value will increase the search latency, but will also improve the recall.\n",
    "\n",
    "In general, we chose configurations that gradually increase the parameters, as detailed in the following table.\n",
    "\n",
    "![](../images/hnsw_parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example using the **FAISS** engine and providing a parameter configuration that provides a balance between latency, memory, and recall (see table above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\",\n",
    "    m=16, # maximum number of edges\n",
    "    ef_construction=128, # size of the candidate queue for edges\n",
    "    ef_search=128 # size of the candidate queue for graph traversal\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance search (MMR)\n",
    "If you’d like to look up for some similar documents, but you’d also like to receive diverse results, MMR is a method you should consider. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = vector_db.max_marginal_relevance_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    fetch_k=10,\n",
    "    lambda_param=0.5\n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parent Document Retrieval\n",
    "The queries above returned just the matching segments from the vector store. Now let's see what happens when we invoke the `ParentDocumentRetriever` that was defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the acceptable model evaluation techniques?\"\n",
    "\n",
    "# search with just the vector store\n",
    "section_segments = vector_db.similarity_search(query, k=10)\n",
    "\n",
    "retriever.search_kwargs = {\"k\": 10}\n",
    "\n",
    "# search with the parent retriever\n",
    "full_sections = retriever.get_relevant_documents(\"What are the acceptable model evaluation techniques?\")\n",
    "\n",
    "rprint(f\"Vector search returned {len(section_segments)} segments while the parent retriever returned {len(full_sections)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we set k=10 which should return 10 matches from the vector store. However the parent document retriever should return fewer documents as it will return the entire section that multiple returned chunks can belong to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrating RAG using LangChain\n",
    "Now that we can query our vector database for documents, we can retrieve data from outside of a large language model's training data sources and augment our prompts by adding the relevant retrieved data in context.\n",
    "\n",
    "We can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses. We can create a Retrieval Augmented Generation (RAG) workflow that introduces new information to the language model during prompting. Implementing context-aware workflows like RAG reduces model hallucination and improves response accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single turn generative question answering\n",
    "\n",
    "Let's start with a simple example where given a user query we retrieve relevant documents from the vector store and use the retrieved documents as context to generate a response.\n",
    "\n",
    "We'll construct a prompt template that will take the user's question and the retrieved documents as context and generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If the context does not provide sufficient information to answer the question, politely indicate that you are unable to assist. \n",
    "Only answer questions related to model risk and model governance.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# in the first step we retrieve the context and pass through the input question\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    \n",
    ")\n",
    "\n",
    "# In the subsequent steps pass the context and question to the prompt, send the prompt to the llm and parse the output as a string\n",
    "chain = setup_and_retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try this with our earlier query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What can be considered a model?\"\n",
    "\n",
    "response = chain .invoke(query)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are some acceptable model evaluation techniques?\"\n",
    "\n",
    "response = chain.invoke(query)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our solution\n",
    "\n",
    "The above solution works but is notably missing a number of key features including:\n",
    "- Ability to have multi-turn conversations\n",
    "- Ability to return source documents\n",
    "- The response are constrained only to what is in the documents which may limit the usefulness of the tool\n",
    "\n",
    "In the following section we will address these gaps and make further enhancements such as utilizing some of the prompting best practices for Claude, and refining the prompt a bit to provide more natural responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up our prompt templates\n",
    "The updated solution will utlize 3 prompt templates including:\n",
    "- **Condense template** that will look at the conversation history and generate a standalone response. This is required as a user's follow up question may not include sufficient context to perform an effective retrieval search. We'd therefore need to ask the model to rephrase the question in such a way that all the necessary context is included.\n",
    "- **Document template** that will format the documents using Claude best practices (i.e. xml tags) before feeding them into the answer template \n",
    "- **Answer template** that will take the user's question and the retrieved documents and generate a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import  get_buffer_string\n",
    "from langchain_core.runnables import  RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template to rephrase the question\n",
    "condense_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "Skip any preamble or summarization, simply generate the rephrased question.\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "Follow Up Input: {question}\n",
    "\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template to generate a response\n",
    "answer_template = \"\"\"You are a foremost expert in model risk and model governance. Your job is to advise users on the best practices and guidelines in these areas.\n",
    "The context below provides relevant information to answer the question. You must use this context to provide a detailed and accurate response.\n",
    "Only answer questions related to model risk and model governance, if a user asks a question about a different topic, politely decline.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template and function to format context documents in xml tags\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"<document section_title={section_title}>{page_content}</document>\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a place to store the conversation state. Here we'll use the in-memory [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer/) to store the conversation history. This will allow us to keep track of the conversation history and use it to generate a standalone response when needed. In practice, you'd likely use something like [DynamoDB](https://python.langchain.com/docs/integrations/memory/aws_dynamodb/) to store the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate a blank memory buffer\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "\n",
    "# First we add a step to load memory from the buffer to feed into the prompt\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "# Next we generate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(\n",
    "            x[\"chat_history\"], human_prefix=\"human\", ai_prefix=\"assistant\"\n",
    "        ),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Retrieve the documents using the generated question\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Construct the inputs for the final prompt with the formatted context docs\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# Send the final prompt to the llm\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"What types of model risks should be documented?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "rprint(result[\"answer\"].content)\n",
    "rprint(\"source documents:\\n\", json.dumps([doc.metadata for doc in  result[\"docs\"]], indent=2))\n",
    "\n",
    "# after every conversation turn we update the conversation state in the memory buffer\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"Can you provide some examples for a fraud detection model?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "rprint(result[\"answer\"].content)\n",
    "rprint(\"source documents:\\n\", json.dumps([doc.metadata for doc in  result[\"docs\"]], indent=2))\n",
    "\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"How about for a credit scoring model?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "print(result[\"answer\"].content)\n",
    "print(\"source documents:\\n\", json.dumps([doc.metadata for doc in  result[\"docs\"]], indent=2))\n",
    "\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "You have reached the end of this workshop. Following cell will delete all created resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_oss_resources(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In the above implementation of RAG based Question Answering and Conversational AI, we have explored the following concepts and how to implement them using the LangChain integrations for Amazon Bedrock and Amazon OpenSearch Serverless:\n",
    "\n",
    "- Loading documents and processing them into smaller chunks\n",
    "- Creating a vector store using vector engine for Amazon OpenSearch Serverless\n",
    "- Generating embeddings with an embeddings model\n",
    "- Searching the vector store to retrieve context relevant to the question\n",
    "- Performing Generative Question Answering using foundation models\n",
    "- Improving trust in our system by providing citations with every answer\n",
    "- Preparing prompt templates to use as input to the LLM\n",
    "- Storing conversation memory and providing the history as context to the LLM\n",
    "\n",
    "### Next steps\n",
    "- Experiment with different vector stores\n",
    "- Leverage various text and embedding models available through Amazon Bedrock to see alternate outputs\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Use Amazon Bedrock Knowledge Bases, a fully managed RAG capability with built-in session context management\n",
    "\n",
    "# Thank You"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
