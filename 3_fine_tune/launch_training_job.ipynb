{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Mistral 7b with Amazon SageMaker\n",
    "In this notebook we'll explore how to fine-tune a [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) model with Amazon SageMaker. We'll use the [Hugging Face](https://huggingface.co/) library to download the model and tokenizer, and we'll use the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) to fine-tune the model on a sample dataset. The goal of this notebook is to cover several key aspects of fine-tuning LLMs including:\n",
    "- Preparing the data for fine-tuning\n",
    "- Obtaining the base model and tokenizer\n",
    "- Configuring a SageMaker training job\n",
    "- Utilizing QLoRA for parameter efficient fine-tuning (PEFT)\n",
    "- Applying supervised fine-tuning methods to train a model\n",
    "- Improving / Aligning the model's outputs with human preferences using Direct Preference Optimization (DPO)\n",
    "\n",
    "We will utilize the [fine-tuning recipes](https://github.com/huggingface/alignment-handbook) provided by Hugging Face that was used to fine-tune the Mistral-7B model to create the [Zephyr-7B-Beta](HuggingFaceH4/zephyr-7b-beta) model.\n",
    "\n",
    "The recipes utilize the [Transformer Reinforcement Learning (TRL)](https://github.com/huggingface/trl) for both supervised fine-tuning and preference alignment and is easy to adapt to other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker\n",
    "%pip install -Uq datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import time\n",
    "from pathlib import Path\n",
    "from utils import download_model\n",
    "\n",
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "smr = boto3_session.client(\"sagemaker-runtime\") # sagemaker runtime client for invoking the endpoint\n",
    "sm = boto3_session.client(\"sagemaker\") \n",
    "s3_rsr = boto3_session.resource(\"s3\")\n",
    "role = sagemaker.get_execution_role()  \n",
    "\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model\n",
    "First, we'll download the model and tokenizer from the Hugging Face model hub and upload them to our own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at Mistral-7B\n",
      "Skipping download\n"
     ]
    }
   ],
   "source": [
    "local_model_path = download_model(\"mistralai/Mistral-7B-v0.1\", \"./Mistral-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model has already been uploaded to the S3 bucket. If not, upload it.\n",
    "model_prefix = local_model_path.name\n",
    "\n",
    "if list(s3_rsr.Bucket(bucket).objects.filter(Prefix=model_prefix)) :\n",
    "    print(\"Model already exists on the S3 bucket\")\n",
    "    print(f\"If you want to upload a new model, please delete the existing model from the S3 bucket with the following command: \\n !aws s3 rm --recursive s3://{bucket}/{model_prefix}\")\n",
    "    s3_model_location = f\"s3://{bucket}/{model_prefix}\"\n",
    "else:\n",
    "    s3_model_location = sess.upload_data(path=local_model_path.as_posix(), bucket=bucket, key_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data and upload to S3\n",
    "Next we need to prepare the data for fine-tuning. We can use a sample of the data that was used to train the Zephy-7B-Beta model or we can bring our own data. \n",
    "\n",
    "If bringing our own data, we need to convert it into a json-lines format that is supported by the TRL trainers. \n",
    "- For Supervised Fine-tuning each record should contain a `messages` field. This field should contain a list of dictionaries that correspond to a conversation between a `user` and an AI `assistant`. The schema is `{\"role\": \"{role}\", \"content\": {content}}` where role is either `user`, `assistant`, or `system` and content is the text of the message. For more information see the recipe documentation [here](https://github.com/huggingface/alignment-handbook/tree/main/scripts) or an example dataset [here](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) \n",
    "- For Direct Preference Optimization we need to provide a dataset that contains `chosen` and `rejected` responses as based on human preference. The schema for this dataset contains `chosen` and `rejected` fields that contain the conversation messages in the same format as the supervised fine-tuning dataset. For more information see the recipe documentation [here](https://github.com/huggingface/alignment-handbook/tree/main/scripts) or an example dataset [here](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n",
    "\n",
    "The tuning recipe will automatically convert the `messages` into a chat prompt that will be used to fine-tune the model. You can see what the default template looks like by visiting the [Zephyr-7B-Beta](HuggingFaceH4/zephyr-7b-beta) model card. The messages will be converted into a chat prompt that separates the system, user, and assistant messages with the following tokens: `<|system|>`, `<|user|>`, and `<|assistant|>` respectively along with an EOS token `</s>` at the end of each message. The template can be adjusted in the tuning script however it is important to keep in mind that the same template should be used during inference and should be well documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daf18b0037a4806bf1c1e4ea1400698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d1ac1f09d745d1897e3855ff9a6978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6323a9b7814d6bb0f533b65f34ef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e066aefb716a4c79a16866338c7bec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "USE_EXAMPLE_DATA = True # set to False to use your own data\n",
    "NUM_SAMPLES = 1200 # number of samples to use from the example data\n",
    "\n",
    "if USE_EXAMPLE_DATA:\n",
    "    sft_dataset = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\")['train_sft'].select(range(NUM_SAMPLES))\n",
    "    dpo_dataset = datasets.load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")['train_prefs'].select(range(NUM_SAMPLES))\n",
    "    \n",
    "# adjust these values if bringing your own data\n",
    "# In the example here, a jsonl file is stored in /data/dpo and /data/sft that contains data in the format described above\n",
    "else:\n",
    "    dpo_dataset_path =\"./data/dpo\"\n",
    "    sft_dataset_path =\"./data/sft\"\n",
    "    try:\n",
    "        sft_dataset = datasets.load_dataset(sft_dataset_path)[\"train\"]\n",
    "        dpo_dataset = datasets.load_dataset(dpo_dataset_path)[\"train\"]\n",
    "    except Exception as e:\n",
    "        print(\"Please make sure that the data is present in the data folder. If not, please prepare the data first\")\n",
    "        raise Exception(e)\n",
    "\n",
    "sft_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).save_to_disk('fine-tuning-data/sft_split')\n",
    "dpo_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).save_to_disk('fine-tuning-data/dpo_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data file to s3://sagemaker-us-east-1-420618410968/fine-tuning-mistral/data\n"
     ]
    }
   ],
   "source": [
    "# upload the data to S3\n",
    "s3_data = sess.upload_data(path=\"fine-tuning-data\", bucket=bucket, key_prefix=\"fine-tuning-mistral/data\")\n",
    "\n",
    "print(f\"Uploaded training data file to {s3_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker Training Job for Supervised Fine-tuning\n",
    "Now that the data is ready, we can configure the first SageMaker training job which will perform supervised fine-tuning. The code from the Hugging Face recipe [repo](https://github.com/huggingface/alignment-handbook/tree/main) is cloned into the `src` directory. The `src` directory also contains a requirements.txt file that will install the recipe module and [Flash Attention](https://github.com/Dao-AILab/flash-attention) to speed up the training.\n",
    "\n",
    "The repo contains two scripts, `alignment-handbook/scripts/run_sft.py` for supervised fine-tuning and `alignment-handbook/scripts/run_dpo.py` for direct preference optimization. Both scripts take a positional argument for the path of the recipe file like this `python run_{task}.py config_full.yaml`. The recipe file contains all of the hyperparameters for the training job. The recipe file for the supervised fine-tuning job is located at `src/config_sft_lora.yaml`. Several example recipe files are available within the repo for full and parameter efficient fine-tuning. We will utilize the parameter efficient fine-tuning recipe for this example.\n",
    "\n",
    "A few changes are required to the recipe file to run the job on SageMaker. First, we need to change the `model_name_or_path` to `/opt/ml/input/data/model`. This is the directory to which our base model will be copied to from S3. Next, we need to change the `dataset_mixer` directories to `/opt/ml/input/data/train` which is where our training data will be copied to from S3. Finally, we need to change the `output_dir` for the `trainer` to `/opt/ml/model` so that the model is saved to the `/opt/ml/model` directory which is the default directory for SageMaker models. The contents of the `/opt/ml/model` will be copied to S3 once the job finishes.  Optionally, we can set the `logging_dir` to `/opt/ml/output/tensorboard` to utilize [SageMaker Managed TensorBoard](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html) for monitoring the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "import time\n",
    "\n",
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/fine-tuning-mistral/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "job_name = f\"mistral7b-sft\"\n",
    "\n",
    "# the default script takes the yaml file as a positional argument\n",
    "# Since sagemaker only supports passing named arguments as hyperparameters, as small change was made to the fine tuning scripts\n",
    "\n",
    "hyperparameters = {\n",
    "    \"recipe\": \"config_sft_lora.yaml\",  # supervised fine-tuning with QLoRA recipe\n",
    "}\n",
    "\n",
    "sft_estimator = PyTorch(\n",
    "    base_job_name=job_name,\n",
    "    source_dir = \"src\",                                  # directory containing the fine-tuning scripts\n",
    "    entry_point=\"alignment-handbook/scripts/run_sft.py\", # fine-tuning script that will be run\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2,                                    # number of instances to use for training \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.1.0\",                          # PyTorch version\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,                    # after job is done keep the training cluster alive for 1 hour to accept other jobs\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}, # enable distributed training with torch.distributed \n",
    "    disable_output_compression = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mistral7b-sft-2024-05-09-17-20-42-888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:20:43 Starting - Starting the training job...\n",
      "2024-05-09 17:20:44 Pending - Training job waiting for capacity.........\n",
      "2024-05-09 17:22:21 Pending - Preparing the instances for training...\n",
      "2024-05-09 17:22:56 Downloading - Downloading input data...\n",
      "2024-05-09 17:23:18 Downloading - Downloading the training image............\n",
      "2024-05-09 17:25:44 Training - Training image download completed. Training in progress.....\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:11,604 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:11,621 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:11,631 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:11,633 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:11,633 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:12,278 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:12,294 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:12,305 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:12,307 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:12,307 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:13,196 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mObtaining file:///opt/ml/code/alignment-handbook (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting flash-attn==2.3.6 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading flash_attn-2.3.6.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 57.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:13,849 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/alignment-handbook (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.3.6 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.3.6.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 77.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (1.11.1.1)\u001b[0m\n",
      "\u001b[35mCollecting accelerate==0.23.0 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes==0.41.2.post2 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.0 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting datasets==2.14.6 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting deepspeed==0.12.2 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.12.2.tar.gz (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 62.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.41.2.post2 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.14.6 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.12.2 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.12.2.tar.gz (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 60.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.14.1 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[35mCollecting peft==0.6.1 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading peft-0.6.1-py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[35mCollecting protobuf<=3.20.2 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.3.3 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (4.66.1)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.35.0 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.1/123.1 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting trl==0.7.4 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading trl-0.7.4-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (5.9.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[35mCollecting dill<0.3.8,>=0.3.0 (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[35mCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.6.1 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.6.1-py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting protobuf<=3.20.2 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (4.66.1)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting hjson (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.6.3)\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.35.0 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.1/123.1 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting trl==0.7.4 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.4-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mCollecting dill<0.3.8,>=0.3.0 (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.6.3)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers==4.35.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 6.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.15,>=0.14 (from transformers==4.35.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting tyro>=0.5.11 (from trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (4.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.35.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 7.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.48.2 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (68.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.0.1)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.15,>=0.14 (from transformers==4.35.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.26.18)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.14.1 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (13.7.1)\u001b[0m\n",
      "\u001b[35mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[35mINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[35mCollecting multiprocess (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (0.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.16.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (68.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.0.1)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.17.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (0.1.2)\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 35.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting multiprocess (from datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.16.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (2.17.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.4->alignment-handbook==0.2.0.dev0->-r requirements.txt (line 2)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 36.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 24.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading peft-0.6.1-py3-none-any.whl (135 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.0/136.0 kB 20.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 101.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading trl-0.7.4-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.9/133.9 kB 17.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 30.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 76.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 99.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading dill-0.3.7-py3-none-any.whl (115 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 22.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 67.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 102.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 18.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.1/774.1 kB 69.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 102.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 96.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.0/102.0 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 8.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 30.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 33.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 23.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 41.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: flash-attn, deepspeed\u001b[0m\n",
      "\u001b[35mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 28.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 51.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.6.1-py3-none-any.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.0/136.0 kB 26.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 109.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.4-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.9/133.9 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 75.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 85.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 110.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 19.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.7-py3-none-any.whl (115 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 22.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 27.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 79.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 102.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 21.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.1/774.1 kB 69.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 108.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 97.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 35.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.0/102.0 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 10.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 25.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 11.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 30.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 34.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 21.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 39.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn, deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56477261 sha256=652ad256d0891cb2c6d7183f96f7f56ff61cdeee24388381abb35e7a0f2eeca1\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56477261 sha256=652ad256d0891cb2c6d7183f96f7f56ff61cdeee24388381abb35e7a0f2eeca1\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265668 sha256=292d8439b820702eb6bf14d4f44b42c93a4b04d9dd3905eb9acbad427d72ba49\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\u001b[0m\n",
      "\u001b[35mSuccessfully built flash-attn deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265671 sha256=61d2917f0df383ed11d2137497b1956a9b37e7a8a2d0151f010011f04aea31e8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn deepspeed\u001b[0m\n",
      "\u001b[35mInstalling collected packages: py-cpuinfo, hjson, bitsandbytes, xxhash, tensorboard-data-server, shtab, safetensors, regex, pynvml, protobuf, multidict, markdown, grpcio, fsspec, frozenlist, docstring-parser, dill, async-timeout, absl-py, yarl, tensorboard, responses, multiprocess, huggingface-hub, aiosignal, tyro, tokenizers, flash-attn, deepspeed, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate, alignment-handbook\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, hjson, bitsandbytes, xxhash, tensorboard-data-server, shtab, safetensors, regex, pynvml, protobuf, multidict, markdown, grpcio, fsspec, frozenlist, docstring-parser, dill, async-timeout, absl-py, yarl, tensorboard, responses, multiprocess, huggingface-hub, aiosignal, tyro, tokenizers, flash-attn, deepspeed, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate, alignment-handbook\u001b[0m\n",
      "\u001b[35mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[35mFound existing installation: protobuf 3.20.3\u001b[0m\n",
      "\u001b[35mUninstalling protobuf-3.20.3:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled protobuf-3.20.3\u001b[0m\n",
      "\u001b[35mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[35mFound existing installation: fsspec 2024.2.0\u001b[0m\n",
      "\u001b[35mUninstalling fsspec-2024.2.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled fsspec-2024.2.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.20.3\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.20.3\u001b[0m\n",
      "\u001b[35mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[35mFound existing installation: dill 0.3.8\u001b[0m\n",
      "\u001b[35mUninstalling dill-0.3.8:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled dill-0.3.8\u001b[0m\n",
      "\u001b[35mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[35mFound existing installation: multiprocess 0.70.16\u001b[0m\n",
      "\u001b[35mUninstalling multiprocess-0.70.16:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled multiprocess-0.70.16\u001b[0m\n",
      "\u001b[35mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[35mFound existing installation: flash-attn 2.0.4\u001b[0m\n",
      "\u001b[35mUninstalling flash-attn-2.0.4:\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2024.2.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2024.2.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.8\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.8\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.16\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.16:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.16\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled flash-attn-2.0.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.0.4\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.0.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.0.4\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[35mRunning setup.py develop for alignment-handbook\u001b[0m\n",
      "\u001b[34mRunning setup.py develop for alignment-handbook\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mpathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\u001b[0m\n",
      "\u001b[35mpathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\n",
      "\u001b[35mtransformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.3.6 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-2.1.0 accelerate-0.23.0 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.2.0.dev0 async-timeout-4.0.3 bitsandbytes-0.41.2.post2 datasets-2.14.6 deepspeed-0.12.2 dill-0.3.7 docstring-parser-0.16 evaluate-0.4.0 flash-attn-2.3.6 frozenlist-1.4.1 fsspec-2023.10.0 grpcio-1.63.0 hjson-3.1.0 huggingface-hub-0.17.3 markdown-3.6 multidict-6.0.5 multiprocess-0.70.15 peft-0.6.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pynvml-11.5.0 regex-2024.4.28 responses-0.18.0 safetensors-0.4.3 shtab-1.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.14.1 transformers-4.35.0 trl-0.7.4 tyro-0.8.3 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\n",
      "\u001b[34mtransformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.3.6 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 accelerate-0.23.0 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.2.0.dev0 async-timeout-4.0.3 bitsandbytes-0.41.2.post2 datasets-2.14.6 deepspeed-0.12.2 dill-0.3.7 docstring-parser-0.16 evaluate-0.4.0 flash-attn-2.3.6 frozenlist-1.4.1 fsspec-2023.10.0 grpcio-1.63.0 hjson-3.1.0 huggingface-hub-0.17.3 markdown-3.6 multidict-6.0.5 multiprocess-0.70.15 peft-0.6.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pynvml-11.5.0 regex-2024.4.28 responses-0.18.0 safetensors-0.4.3 shtab-1.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.14.1 transformers-4.35.0 trl-0.7.4 tyro-0.8.3 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,546 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,546 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,581 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,609 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,621 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,638 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:26:49,650 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"recipe\": \"config_sft_lora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"mistral7b-sft-2024-05-09-17-20-42-888\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-420618410968/mistral7b-sft-2024-05-09-17-20-42-888/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"alignment-handbook/scripts/run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"alignment-handbook/scripts/run_sft.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"recipe\":\"config_sft_lora.yaml\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=alignment-handbook/scripts/run_sft.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=alignment-handbook/scripts/run_sft\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-420618410968/mistral7b-sft-2024-05-09-17-20-42-888/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"recipe\":\"config_sft_lora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"mistral7b-sft-2024-05-09-17-20-42-888\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-420618410968/mistral7b-sft-2024-05-09-17-20-42-888/source/sourcedir.tar.gz\",\"module_name\":\"alignment-handbook/scripts/run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"alignment-handbook/scripts/run_sft.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--recipe\",\"config_sft_lora.yaml\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_RECIPE=config_sft_lora.yaml\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 1 alignment-handbook/scripts/run_sft.py --recipe config_sft_lora.yaml\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,846 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,846 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,882 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,909 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,922 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,938 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:26:49,950 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"recipe\": \"config_sft_lora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"mistral7b-sft-2024-05-09-17-20-42-888\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-420618410968/mistral7b-sft-2024-05-09-17-20-42-888/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"alignment-handbook/scripts/run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"alignment-handbook/scripts/run_sft.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"recipe\":\"config_sft_lora.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=alignment-handbook/scripts/run_sft.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=alignment-handbook/scripts/run_sft\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-420618410968/mistral7b-sft-2024-05-09-17-20-42-888/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"recipe\":\"config_sft_lora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"mistral7b-sft-2024-05-09-17-20-42-888\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-420618410968/mistral7b-sft-2024-05-09-17-20-42-888/source/sourcedir.tar.gz\",\"module_name\":\"alignment-handbook/scripts/run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"alignment-handbook/scripts/run_sft.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--recipe\",\"config_sft_lora.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_RECIPE=config_sft_lora.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 0 alignment-handbook/scripts/run_sft.py --recipe config_sft_lora.yaml\u001b[0m\n",
      "\u001b[35mThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\u001b[0m\n",
      "\u001b[35m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m[2024-05-09 17:27:01,511] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m['alignment-handbook/scripts/run_sft.py', '--recipe', 'config_sft_lora.yaml']\u001b[0m\n",
      "\u001b[35m['alignment-handbook/scripts/run_sft.py', 'config_sft_lora.yaml']\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/opt/ml/input/data/model', model_revision='main', model_code_revision=None, torch_dtype='auto', trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=64, lora_alpha=16, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - __main__ - Data parameters DataArguments(chat_template=None, dataset_mixer={'/opt/ml/input/data/train': 1.0}, dataset_splits=['train', 'test'], max_train_samples=None, max_eval_samples=None, preprocessing_num_workers=2, truncation_side=None)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - __main__ - Training/evaluation parameters SFTConfig(\u001b[0m\n",
      "\u001b[35m_n_gpu=1,\u001b[0m\n",
      "\u001b[35madafactor=False,\u001b[0m\n",
      "\u001b[35madam_beta1=0.9,\u001b[0m\n",
      "\u001b[35madam_beta2=0.999,\u001b[0m\n",
      "\u001b[35madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[35mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[35mbf16=True,\u001b[0m\n",
      "\u001b[35mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[35mdata_seed=None,\u001b[0m\n",
      "\u001b[35mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[35mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[35mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[35mddp_backend=None,\u001b[0m\n",
      "\u001b[35mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[35mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[35mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[35mddp_timeout=1800,\u001b[0m\n",
      "\u001b[35mdebug=[],\u001b[0m\n",
      "\u001b[35mdeepspeed=None,\u001b[0m\n",
      "\u001b[35mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[35mdispatch_batches=None,\u001b[0m\n",
      "\u001b[35mdo_eval=True,\u001b[0m\n",
      "\u001b[35mdo_predict=False,\u001b[0m\n",
      "\u001b[35mdo_train=False,\u001b[0m\n",
      "\u001b[35meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[35meval_delay=0,\u001b[0m\n",
      "\u001b[35meval_steps=None,\u001b[0m\n",
      "\u001b[35mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[35mfp16=False,\u001b[0m\n",
      "\u001b[35mfp16_backend=auto,\u001b[0m\n",
      "\u001b[35mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[35mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[35mfsdp=[],\u001b[0m\n",
      "\u001b[35mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[35mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[35mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[35mfull_determinism=False,\u001b[0m\n",
      "\u001b[35mgradient_accumulation_steps=8,\u001b[0m\n",
      "\u001b[35mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[35mgradient_checkpointing_kwargs={'use_reentrant': False},\u001b[0m\n",
      "\u001b[35mgreater_is_better=None,\u001b[0m\n",
      "\u001b[35mgroup_by_length=False,\u001b[0m\n",
      "\u001b[35mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[35mhub_always_push=False,\u001b[0m\n",
      "\u001b[35mhub_model_id=zephyr-7b-sft-lora,\u001b[0m\n",
      "\u001b[35mhub_private_repo=False,\u001b[0m\n",
      "\u001b[35mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[35mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[35mignore_data_skip=False,\u001b[0m\n",
      "\u001b[35minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[35minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[35mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[35mlabel_names=None,\u001b[0m\n",
      "\u001b[35mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[35mlearning_rate=2e-05,\u001b[0m\n",
      "\u001b[35mlength_column_name=length,\u001b[0m\n",
      "\u001b[35mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[35mlocal_rank=0,\u001b[0m\n",
      "\u001b[35mlog_level=info,\u001b[0m\n",
      "\u001b[35mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[35mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[35mlogging_dir=/opt/ml/output/tensorboard,\u001b[0m\n",
      "\u001b[35mlogging_first_step=True,\u001b[0m\n",
      "\u001b[35mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[35mlogging_steps=5,\u001b[0m\n",
      "\u001b[35mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[35mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[35mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[35mmax_seq_length=2048,\u001b[0m\n",
      "\u001b[35mmax_steps=-1,\u001b[0m\n",
      "\u001b[35mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[35mmp_parameters=,\u001b[0m\n",
      "\u001b[35mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[35mno_cuda=False,\u001b[0m\n",
      "\u001b[35mnum_train_epochs=1,\u001b[0m\n",
      "\u001b[35moptim=adamw_bnb_8bit,\u001b[0m\n",
      "\u001b[35moptim_args=None,\u001b[0m\n",
      "\u001b[35moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[35moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[35mpast_index=-1,\u001b[0m\n",
      "\u001b[35mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[35mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[35mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[35mpush_to_hub=False,\u001b[0m\n",
      "\u001b[35mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[35mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[35mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[35mray_scope=last,\u001b[0m\n",
      "\u001b[35mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[35mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[35mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[35mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[35msave_on_each_node=False,\u001b[0m\n",
      "\u001b[35msave_safetensors=True,\u001b[0m\n",
      "\u001b[35msave_steps=500,\u001b[0m\n",
      "\u001b[35msave_strategy=no,\u001b[0m\n",
      "\u001b[35msave_total_limit=None,\u001b[0m\n",
      "\u001b[35mseed=42,\u001b[0m\n",
      "\u001b[35mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[35msplit_batches=False,\u001b[0m\n",
      "\u001b[35mtf32=None,\u001b[0m\n",
      "\u001b[35mtorch_compile=False,\u001b[0m\n",
      "\u001b[35mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[35mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[35mtorchdynamo=None,\u001b[0m\n",
      "\u001b[35mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[35mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[35muse_cpu=False,\u001b[0m\n",
      "\u001b[35muse_ipex=False,\u001b[0m\n",
      "\u001b[35muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[35muse_mps_device=False,\u001b[0m\n",
      "\u001b[35mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[35mwarmup_steps=0,\u001b[0m\n",
      "\u001b[35mweight_decay=0.0,\u001b[0m\n",
      "\u001b[35m)\u001b[0m\n",
      "\u001b[35mUsing custom data configuration default-32e503b24157b8ce\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.builder - Using custom data configuration default-32e503b24157b8ce\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[35mLoading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[35mGenerating dataset train (/root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.builder - Generating dataset train (/root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[35mDownloading and preparing dataset train/default to /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.builder - Downloading and preparing dataset train/default to /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\u001b[0m\n",
      "\u001b[34m[2024-05-09 17:27:01,396] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m['alignment-handbook/scripts/run_sft.py', '--recipe', 'config_sft_lora.yaml']\u001b[0m\n",
      "\u001b[34m['alignment-handbook/scripts/run_sft.py', 'config_sft_lora.yaml']\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/opt/ml/input/data/model', model_revision='main', model_code_revision=None, torch_dtype='auto', trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=64, lora_alpha=16, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - __main__ - Data parameters DataArguments(chat_template=None, dataset_mixer={'/opt/ml/input/data/train': 1.0}, dataset_splits=['train', 'test'], max_train_samples=None, max_eval_samples=None, preprocessing_num_workers=2, truncation_side=None)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - __main__ - Training/evaluation parameters SFTConfig(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=8,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs={'use_reentrant': False},\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=zephyr-7b-sft-lora,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=2e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=info,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/output/tensorboard,\u001b[0m\n",
      "\u001b[34mlogging_first_step=True,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=5,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_seq_length=2048,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1,\u001b[0m\n",
      "\u001b[34moptim=adamw_bnb_8bit,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=no,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-0c023cd315deaf29\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Using custom data configuration default-0c023cd315deaf29\u001b[0m\n",
      "\u001b[34mLoading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[34mGenerating dataset train (/root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Generating dataset train (/root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset train/default to /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Downloading and preparing dataset train/default to /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\u001b[0m\n",
      "\u001b[34mDataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 13294.15it/s]\u001b[0m\n",
      "\u001b[34mDownloading took 0.0 min\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34mChecksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2181.69it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Generating train split\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00, 294.30 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Generating test split\u001b[0m\n",
      "\u001b[34mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 1 examples [00:00, 782.08 examples/s]\u001b[0m\n",
      "\u001b[34mUnable to verify splits sizes.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset train downloaded and prepared to /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Dataset train downloaded and prepared to /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-0c023cd315deaf29\u001b[0m\n",
      "\u001b[34mLoading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Using custom data configuration default-0c023cd315deaf29\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[34mOverwrite dataset info from restored data version if exists.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\u001b[0m\n",
      "\u001b[34mLoading Dataset info from /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[34mFound cached dataset train (/root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.builder - Found cached dataset train (/root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[34mLoading Dataset info from /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/train/default-0c023cd315deaf29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[34mCaching indices mapping at /opt/ml/input/data/train/train/cache-77d17b26a8f294cc.arrow\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/train/cache-77d17b26a8f294cc.arrow\u001b[0m\n",
      "\u001b[34mCaching indices mapping at /opt/ml/input/data/train/test/cache-9c367c1bd510e9e5.arrow\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/test/cache-9c367c1bd510e9e5.arrow\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - __main__ - Training on the following datasets and their proportions: ['train : 1080', 'test : 120']\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:01,911 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1080 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mCaching processed dataset at /opt/ml/input/data/train/train/cache-5ab55f255d3e61ec.arrow\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-5ab55f255d3e61ec.arrow\u001b[0m\n",
      "\u001b[35mDataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[35mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 17403.75it/s]\u001b[0m\n",
      "\u001b[35mDownloading took 0.0 min\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[35mChecksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[35mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2163.13it/s]\u001b[0m\n",
      "\u001b[35mGenerating train split\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.builder - Generating train split\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 1 examples [00:00, 326.76 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating test split\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.builder - Generating test split\u001b[0m\n",
      "\u001b[35mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating test split: 1 examples [00:00, 708.62 examples/s]\u001b[0m\n",
      "\u001b[35mUnable to verify splits sizes.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[35mDataset train downloaded and prepared to /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.builder - Dataset train downloaded and prepared to /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35mUsing custom data configuration default-32e503b24157b8ce\u001b[0m\n",
      "\u001b[35mLoading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.builder - Using custom data configuration default-32e503b24157b8ce\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[35mOverwrite dataset info from restored data version if exists.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\u001b[0m\n",
      "\u001b[35mLoading Dataset info from /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[35mFound cached dataset train (/root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.builder - Found cached dataset train (/root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\u001b[0m\n",
      "\u001b[35mLoading Dataset info from /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/train/default-32e503b24157b8ce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\u001b[0m\n",
      "\u001b[35mCaching indices mapping at /opt/ml/input/data/train/train/cache-77d17b26a8f294cc.arrow\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/train/cache-77d17b26a8f294cc.arrow\u001b[0m\n",
      "\u001b[35mCaching indices mapping at /opt/ml/input/data/train/test/cache-9c367c1bd510e9e5.arrow\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/test/cache-9c367c1bd510e9e5.arrow\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - __main__ - Training on the following datasets and their proportions: ['train : 1080', 'test : 120']\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2020] 2024-05-09 17:27:02,034 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1080 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mCaching processed dataset at /opt/ml/input/data/train/train/cache-5ab55f255d3e61ec.arrow\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-5ab55f255d3e61ec.arrow\u001b[0m\n",
      "\u001b[35mMap:  42%|████▏     | 449/1080 [00:00<00:00, 4446.33 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  86%|████████▌ | 927/1080 [00:00<00:00, 4636.92 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1080/1080 [00:00<00:00, 3595.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/120 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mCaching processed dataset at /opt/ml/input/data/train/test/cache-0dc0a49b6ee54044.arrow\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-0dc0a49b6ee54044.arrow\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 120/120 [00:00<00:00, 3592.06 examples/s]\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - __main__ - Sample 228 of the processed training set:\u001b[0m\n",
      "\u001b[35m<|system|>\u001b[0m\n",
      "\u001b[35m</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mWrite a comprehensive proposal outlining the objectives, strategies, and expected outcomes for implementing a new employee wellness program that will promote physical, mental, and emotional health in the workplace. The proposal should include specific interventions, timelines, budget estimates, and metrics for measuring the success of the program. Additionally, it should address any potential barriers and offer solutions to overcome them. The proposal should be presented in a professional and persuasive style, utilizing evidence-based practices and research to support its recommendations.</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mExecutive Summary:\u001b[0m\n",
      "\u001b[35mThe proposal aims to introduce a wellness program in our company that promotes the physical, mental, and emotional well-being of employees. Research has shown that such programs lead to increased job satisfaction, employee engagement, and productivity, and reduce healthcare costs. After conducting a needs assessment and analyzing data, we have identified three focus areas: physical activity, mental health, and nutrition. Based on the evidence from research and best practices, we have developed a comprehensive program that includes various interventions and strategies. The program will be implemented in three phases over one year, with a budget of $250,000. The outcome measures include the reduction of absenteeism, healthcare costs, and stress levels, and the improvement of employee engagement, morale, and productivity. The proposal also addresses potential barriers and suggests solutions to overcome them.\u001b[0m\n",
      "\u001b[35mObjectives:\u001b[0m\n",
      "\u001b[35mThe objectives of the employee wellness program are:\u001b[0m\n",
      "\u001b[35m• To promote the physical, mental, and emotional well-being of employees\u001b[0m\n",
      "\u001b[35m• To reduce absenteeism and presenteeism due to illness or stress\u001b[0m\n",
      "\u001b[35m• To improve employee engagement, morale, and productivity\u001b[0m\n",
      "\u001b[35m• To reduce healthcare costs and insurance premiums\u001b[0m\n",
      "\u001b[35m• To enhance the company's reputation as an employer of choice\u001b[0m\n",
      "\u001b[35m• To create a culture of health and wellness in the workplace\u001b[0m\n",
      "\u001b[35mStrategies and Interventions:\u001b[0m\n",
      "\u001b[35mPhysical Activity:\u001b[0m\n",
      "\u001b[35mThe physical activity component of the wellness program aims to encourage employees to engage in regular exercise and increase their physical activity levels. The following interventions will be implemented:\u001b[0m\n",
      "\u001b[35m• Provide on-site fitness classes, such as yoga, pilates, and Zumba, at least twice a week\u001b[0m\n",
      "\u001b[35m• Offer standing desks, treadmill workstations, and other ergonomic equipment to promote movement and reduce sedentary behavior\u001b[0m\n",
      "\u001b[35m• Organize walking challenges, such as step counting competitions, with prizes for the winners\u001b[0m\n",
      "\u001b[35m• Provide healthy snacks in the break room, such as fresh fruit, granola bars, and nuts, to fuel physical activity\u001b[0m\n",
      "\u001b[35mMental Health:\u001b[0m\n",
      "\u001b[35mThe mental health component of the wellness program aims to promote psychological well-being, reduce stress levels, and increase resilience. The following interventions will be implemented:\u001b[0m\n",
      "\u001b[35m• Provide access to counseling services, either in-person or via teletherapy, for employees who need support\u001b[0m\n",
      "\u001b[35m• Organize stress management workshops, such as meditation, mindfulness, and deep breathing exercises\u001b[0m\n",
      "\u001b[35m• Offer mental health first aid training to managers and supervisors to enable them to identify and support employees who may be struggling\u001b[0m\n",
      "\u001b[35m• Provide resources and information on mental health, such as self-help books, apps, and websites\u001b[0m\n",
      "\u001b[35mNutrition:\u001b[0m\n",
      "\u001b[35mThe nutrition component of the wellness program aims to promote healthy eating habits and reduce unhealthy behaviors, such as junk food consumption and overeating. The following interventions will be implemented:\u001b[0m\n",
      "\u001b[35m• Offer healthy food options in the cafeteria, such as salads, vegetable-based dishes, and lean protein sources\u001b[0m\n",
      "\u001b[35m• Promote healthy eating through educational workshops and seminars on topics such as meal planning, portion control, and mindful eating\u001b[0m\n",
      "\u001b[35m• Provide healthy snack options in vending machines, such as fruit, trail mix, and low-fat popcorn\u001b[0m\n",
      "\u001b[35m• Offer nutrition counseling and coaching services for employees who want to make dietary improvements\u001b[0m\n",
      "\u001b[35mTimelines:\u001b[0m\n",
      "\u001b[35mThe employee wellness program will be implemented in three phases over one year:\u001b[0m\n",
      "\u001b[35mPhase 1: Needs Assessment and Planning (Month 1-2)\u001b[0m\n",
      "\u001b[35m• Conduct an employee survey to gather data on health behaviors, preferences, and needs\u001b[0m\n",
      "\u001b[35m• Analyze data and identify focus areas and target populations\u001b[0m\n",
      "\u001b[35m• Develop the program components, interventions, and strategies\u001b[0m\n",
      "\u001b[35m• Establish a wellness committee to oversee the program\u001b[0m\n",
      "\u001b[35mPhase 2: Implementation (Month 3-9)\u001b[0m\n",
      "\u001b[35m• Roll out the program interventions and strategies in the three focus areas\u001b[0m\n",
      "\u001b[35m• Offer ongoing education and support to employees on wellness topics\u001b[0m\n",
      "\u001b[35m• Evaluate the program's effectiveness and make adjustments as needed\u001b[0m\n",
      "\u001b[35mPhase 3: Maintenance and Evaluation (Month 10-12)\u001b[0m\n",
      "\u001b[35m• Continue to offer the program interventions and monitor results\u001b[0m\n",
      "\u001b[35m• Collect and analyze program data to evaluate outcomes and make recommendations for future improvements\u001b[0m\n",
      "\u001b[35m• Celebrate successes and recognize employee achievements\u001b[0m\n",
      "\u001b[35mBudget Estimates:\u001b[0m\n",
      "\u001b[35mThe total budget for the employee wellness program is $250,000. The breakdown of expenses is as follows:\u001b[0m\n",
      "\u001b[35m• Fitness classes and equipment: $80,000\u001b[0m\n",
      "\u001b[35m• Counseling services and mental health first aid training: $60,000\u001b[0m\n",
      "\u001b[35m• Nutrition counseling and healthy food options: $40,000\u001b[0m\n",
      "\u001b[35m• Program materials and resources: $20,000\u001b[0m\n",
      "\u001b[35m• Program evaluation and data analysis: $30,000\u001b[0m\n",
      "\u001b[35m• Wellness committee and staff time: $20,000\u001b[0m\n",
      "\u001b[35mMetrics for Measuring Success:\u001b[0m\n",
      "\u001b[35mThe outcome measures for the program include:\u001b[0m\n",
      "\u001b[35m• Reduction of absenteeism and presenteeism due to illness or stress\u001b[0m\n",
      "\u001b[35m• Reduction of healthcare costs and insurance premiums\u001b[0m\n",
      "\u001b[35m• Improvement in employee engagement, morale, and productivity\u001b[0m\n",
      "\u001b[35m• Improvement in health behaviors, such as physical activity and healthy eating\u001b[0m\n",
      "\u001b[35m• Reduction of stress levels\u001b[0m\n",
      "\u001b[35mPotential Barriers and Solutions:\u001b[0m\n",
      "\u001b[35mPotential barriers to the success of the program include:\u001b[0m\n",
      "\u001b[35m• Lack of employee participation and motivation\u001b[0m\n",
      "\u001b[35m• Resistance from managers and supervisors\u001b[0m\n",
      "\u001b[35m• Budget constraints\u001b[0m\n",
      "\u001b[35mThe following solutions are recommended to overcome these barriers:\u001b[0m\n",
      "\u001b[35m• Use incentives, such as prizes and rewards,</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mThanks for generating such a comprehensive proposal for the employee wellness program. Could you please provide some more information on how the program will be evaluated and how we can measure its success?</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mSure! Evaluation and metrics are critical to determining if the employee wellness program is successful and aligns with its objectives. Here are some ways to measure the success of the program:\u001b[0m\n",
      "\u001b[35m1. Absenteeism and Presenteeism: The program's success can be measured by tracking the number of days of absenteeism and presenteeism due to stress or illness. Keep a record of the data for each quarter and compare it to previous years to get an accurate picture of the program's effectiveness.\u001b[0m\n",
      "\u001b[35m2. Health Behaviors: The program can be evaluated by monitoring changes in health behaviors such as physical activity levels, healthy eating, and stress management strategies. This can be done by conducting surveys and using activity trackers to monitor employee activity levels.\u001b[0m\n",
      "\u001b[35m3. Healthcare Costs and Insurance premiums: The healthcare costs and insurance premiums can be analyzed to compare the ROI of implementing the employee wellness program with those of other years, to see if the program results in reduced healthcare costs.\u001b[0m\n",
      "\u001b[35m4. Employee Engagement and Productivity: Employee satisfaction surveys can be conducted to evaluate their experiences of the program, and productivity can be measured by tracking the number of positive workdays.\u001b[0m\n",
      "\u001b[35m5. Compliance with Participation: The number of consumer engagement in the program should be also monitored to track adherence and compliance with the program.\u001b[0m\n",
      "\u001b[35m6. Specific Program Results: Specific program measures can be developed to evaluate each component of the wellness program. For instance, the number of employees participating in a stress management workshop can be evaluated, similarly, the percentage of employees who joined fitness classes every week, employees who made dietary changes, and other changes in wellness behavior can be tracked.\u001b[0m\n",
      "\u001b[35mOverall, the program's outcomes will be measured using several metrics to determine how successful it is in terms of reaching the objectives defined. An adequate evaluation plan is critical when implementing the program to ensure that interventions and strategies are relevant and effective.</s>\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - __main__ - Sample 51 of the processed training set:\u001b[0m\n",
      "\u001b[35m<|system|>\u001b[0m\n",
      "\u001b[35m</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mWrite a 5 to 7-page memoir about the role parental pressure played in your eating disorder. Please include specific examples of times when you felt pressured to meet certain expectations related to your weight, appearance, or diet. Additionally, consider exploring the emotional impact of this pressure on your relationship with your parents and how it may have affected other areas of your life. Use a conversational tone and feel free to incorporate personal anecdotes and reflections as appropriate.</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mAs I sit down to write this memoir about my eating disorder, I can't help but feel a sense of vulnerability. Talking about my struggles with food and my body is not easy, especially when it comes to the role that my parents played in the development of my disorder. However, I know that sharing my story can help others who may be going through similar struggles.\u001b[0m\n",
      "\u001b[35mGrowing up, my parents placed a lot of importance on being thin and physically attractive. My mom was always dieting, counting calories and obsessing over her weight. She would make comments about her own body, saying things like \"I have to lose 10 pounds before my reunion\" or \"I can't believe how big my thighs look in these jeans.\" Her constant fixation on her appearance made me feel as though it was the most important thing in the world.\u001b[0m\n",
      "\u001b[35mWhen I was around 12, I started to gain weight. My mom would make comments about my body, saying things like \"you're getting a little chubby\" or \"maybe you should skip dessert tonight.\" My dad, too, would make comments about my weight, though he was less direct than my mom. He would suggest that I take up a sport or activity to \"stay in shape,\" or that I should \"watch what I eat\" to avoid gaining more weight.\u001b[0m\n",
      "\u001b[35mTheir comments affected me deeply. I started to feel as though my body was always under scrutiny, as though I wasn't good enough just the way I was. I started to feel ashamed of myself, and my confidence plummeted. Food became my refuge, my way of coping with the stress and anxiety that came with feeling like I was constantly failing.\u001b[0m\n",
      "\u001b[35mAt first, I would sneak treats and hide them in my room. I would hoard candy and chips, eating them when no one was around. But soon, my eating spiraled out of control. I would binge eat entire containers of ice cream, eat whole pizza pies by myself, or consume entire bags of chips in one sitting. I felt out of control, as though my hunger was a monster that I couldn't tame. I hated myself for how much I was eating and how little control I had over it.\u001b[0m\n",
      "\u001b[35mMy parents started to notice that I was gaining weight rapidly, and they became more vocal about it. My mom started to make more pointed comments about my size, saying things like \"are you sure you need that second helping?\" or \"maybe you should start watching your weight.\" My dad would comment on my eating habits, asking why I was always going back for more food or questioning why I wasn't losing weight despite my \"healthy\" diet.\u001b[0m\n",
      "\u001b[35mTheir comments made me feel even more ashamed and out of control. I started to feel like my body was the source of all my problems, that my weight was what defined me. I became more and more isolated, preferring to spend time alone rather than with my family. I stopped participating in activities I used to enjoy, like sports or music, because I felt like I didn't deserve to enjoy them since I was overweight.\u001b[0m\n",
      "\u001b[35mAs I got older, my eating disorder morphed into different forms. Sometimes I would restrict my eating, trying to be as skinny as possible. Other times I would purge, making myself throw up after meals or taking laxatives to get rid of the food I had eaten. My parents started to notice that something was wrong, but they didn't understand what was happening.\u001b[0m\n",
      "\u001b[35mWhen they asked me about my eating habits, I would lie and say that everything was fine. I didn't know how to explain to them that their comments and pressure had caused me to spiral into this pattern of self-destructive behavior. I didn't know how to ask for help, or how to tell them that I needed them to stop commenting on my body and my eating habits.\u001b[0m\n",
      "\u001b[35mIt took many years and a lot of therapy to undo the damage that my parents' pressure and comments had caused. But even now, years later, I still struggle with feeling like my body isn't good enough. I still have moments where I feel like I need to restrict my eating or purge after a meal. But I know now that these feelings aren't my fault and that I deserve to be happy and healthy no matter what my body looks like.\u001b[0m\n",
      "\u001b[35mLooking back, I wish my parents had known how much their comments hurt me, how they caused me to develop an unhealthy relationship with food and my body. I wish they had known that their love and acceptance was more important than any number on a scale or the size of my clothing. But I also know that they were doing the best they could with the tools they had, and that they loved me to the best of their ability.\u001b[0m\n",
      "\u001b[35mNow, I am working on healing and finding ways to love and accept my body. It's not easy, but it's worth it. I hope that by sharing my story, I can help others who may be struggling with similar issues. Remember, you are more than your body, and you deserve to be happy and healthy no matter what.</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mIt was very insightful and relatable. Could you add more information about how your eating disorder affected your mental health and daily life?</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mSure, I'd be happy to add more about how my eating disorder affected my mental health and daily life. As my eating disorder progressed, it began to take a toll on my mental health. I became increasingly anxious and depressed, feeling as though I was always on edge or in a low mood. I started to feel ashamed of myself, almost as though I was a failure because I couldn't control my eating habits. These negative emotions began to consume me, making it difficult to focus on anything else. I found myself obsessing about my weight and my body image, constantly comparing myself to others and feeling like I didn't measure up. This, in turn, made it harder to interact with people or participate in activities that I used to enjoy. For example, I started to become more withdrawn, spending more time alone and avoiding social situations. I didn't want to go out to eat with friends or go to parties because I was afraid of being judged or being tempted by food. This sense of isolation only made my mental health worse, as I became more and more stuck in my own negative thoughts. My daily life was also affected in more practical ways. My eating disorder made it difficult for me to maintain a healthy diet or exercise routine. I was either overeating or undereating, never finding a balance that worked for me. This also meant that I struggled with maintaining a healthy weight, which only perpetuated the negative feelings I had about my body. Additionally, my eating disorder affected my ability to concentrate and be productive. When I was in the midst of a binge or purge episode, I couldn't focus on anything else. I would procrastinate on schoolwork or sit at my desk for hours, unable to get anything done. This, again, made me feel like a failure and only added to my stress and anxiety. Overall, my eating disorder was a pervasive presence in my life, affecting everything from my mental health to my daily routines. It took a lot of time and effort to overcome, but I'm grateful to be in a better place now.</s>\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:02 - INFO - __main__ - Sample 563 of the processed training set:\u001b[0m\n",
      "\u001b[35m<|system|>\u001b[0m\n",
      "\u001b[35m</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mExplain the cultural significance of the symbolism behind the leaning Tower of Pisa in Italian art history, and provide examples of how it has been represented in paintings, literature and other media throughout the centuries.</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mThe Leaning Tower of Pisa has been a symbol of Italian art and culture for centuries. The significance of this leaning tower lies in its unique architectural design, which has come to represent the ingenuity, innovation, and creative spirit of the Italian people.\u001b[0m\n",
      "\u001b[35mIn Italian art history, the tower has been a prominent subject in paintings and other forms of media, often serving as a metaphor for the human condition. Artists have used the tower to convey a range of themes, including the fragility of human existence, the passing of time, and the struggles that people face in life.\u001b[0m\n",
      "\u001b[35mOne of the most famous examples of the Leaning Tower in art is the painting by Giovanni Paolo Panini, \"Interior of a Picture Gallery with the Collection of Cardinal Silvio Valenti Gonzaga.\" This painting depicts the tower in the background, leaning dramatically over the cityscape. The painting is significant because it demonstrates the Leaning Tower's association with the past and the idea of Italy as a nation with a rich cultural heritage.\u001b[0m\n",
      "\u001b[35mIn literature, the Leaning Tower of Pisa has been referenced countless times over the years. In Dante Alighieri's famous work, \"The Divine Comedy,\" the tower is mentioned as a symbol of sin and evil, and as a warning against the dangers of pride and arrogance.\u001b[0m\n",
      "\u001b[35mOverall, the cultural significance of the Leaning Tower of Pisa lies in its metaphorical power as a symbol of human struggle and achievement. It has been represented in countless works of art and literature throughout the centuries, serving as a reminder of the importance of innovation, creativity, and perseverance in the face of adversity.</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mThat's a really interesting explanation! Could you give me some more examples of how the Leaning Tower of Pisa has been represented in literature besides Dante's \"The Divine Comedy\"? And also, are there any other famous paintings or artworks that feature the Tower?</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mYes, of course! The Leaning Tower of Pisa has been referenced in literature beyond Dante's \"The Divine Comedy.\" For example, in William Shakespeare's play \"Twelfth Night,\" the character Sir Andrew Aguecheek compares his dancing to the leaning tower in order to mock himself:\u001b[0m\n",
      "\u001b[35m\"I can see no rhyme in the reason why I should dance \u001b[0m\n",
      "\u001b[35mbarefoot to the tower like a peasant.\"\u001b[0m\n",
      "\u001b[35mAdditionally, the tower has been described in works by other authors such as Giuseppe Giusti and Giovanni Boccaccio.\u001b[0m\n",
      "\u001b[35mIn terms of paintings and artworks, the Tower has been featured in many works throughout the centuries. One of the most famous is the \"View of Pisa\" by Joachim Patinir, which was painted around 1520. The painting shows the Tower prominently, leaning in the background over the city, and it is believed to be among the first depictions of the Tower in art.\u001b[0m\n",
      "\u001b[35mAnother famous work is \"The Leaning Tower of Pisa\" by Eugène Delacroix. This painting depicts the Tower from a dramatic, low angle and captures the sense of precariousness and instability that is associated with the structure. \u001b[0m\n",
      "\u001b[35mIn contemporary art, the Tower has also been used as a subject, such as in David Mach's sculpture \"Big Heids in Pisa\" which features giant matchstick figures leaning against the Tower.\u001b[0m\n",
      "\u001b[35mOverall, the Leaning Tower of Pisa has been a popular subject in literature and visual art throughout the years, and it continues to captivate and inspire people from all over the world.</s>\u001b[0m\n",
      "\u001b[35m<|user|>\u001b[0m\n",
      "\u001b[35mCould you also tell me some fun facts about the Leaning Tower of Pisa? It would be interesting to learn more about it beyond its cultural significance.</s>\u001b[0m\n",
      "\u001b[35m<|assistant|>\u001b[0m\n",
      "\u001b[35mSure, here are some interesting facts about the Leaning Tower of Pisa:\u001b[0m\n",
      "\u001b[35m1. The Tower started leaning shortly after construction began in 1173. It is believed that the ground was not stable enough to support the weight of the structure and caused it to sink on one side. 2. The Tower is not alone. It is actually one of four buildings that make up the Piazza dei Miracoli (Square of Miracles) complex in Pisa, Italy. The others include the Cathedral, the Baptistery, and the Camposanto Monumentale. 3. The Tower is not perfectly straight, but rather, it has a slight curve. The architects corrected the angle of the Tower during construction to try to compensate for its leaning position, but this actually caused the Tower to have a bit of a curve in its shape. 4. The Tower stands at 56 meters (183.3 feet) tall and has a weight of approximately 14,500 tons. 5. The construction of the Tower took over 200 years to complete. It was originally supposed to be completed in just two decades, but delays and interruptions caused the project to stretch out over centuries. 6. The Tower was almost destroyed during World War II when the German army occupied the area. Luckily, an American Army Sergeant named Leon Weckstein recognized the cultural and historical significance of the building and made sure that Allied forces did not bomb it. 7. In 2008, the Tower was fitted with a set of 11 counterweights to help stabilize it and prevent further leaning. These weights were installed under the base of the Tower and have been successful in reducing the amount of leaning. 8. The Leaning Tower of Pisa has been featured in many pop culture references, such as in video games, movies, and television shows. Some examples include the video game \"Assassin's Creed II,\" the movie \"The Italian Job,\" and the TV show \"The Simpsons.\" I hope you found those facts interesting!</s>\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 451/1080 [00:00<00:00, 4473.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▍ | 915/1080 [00:00<00:00, 4561.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1080/1080 [00:00<00:00, 3526.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/120 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mCaching processed dataset at /opt/ml/input/data/train/test/cache-0dc0a49b6ee54044.arrow\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-0dc0a49b6ee54044.arrow\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 120/120 [00:00<00:00, 3471.41 examples/s]\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:02 - INFO - __main__ - Sample 228 of the processed training set:\u001b[0m\n",
      "\u001b[34m<|system|>\u001b[0m\n",
      "\u001b[34m</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mWrite a comprehensive proposal outlining the objectives, strategies, and expected outcomes for implementing a new employee wellness program that will promote physical, mental, and emotional health in the workplace. The proposal should include specific interventions, timelines, budget estimates, and metrics for measuring the success of the program. Additionally, it should address any potential barriers and offer solutions to overcome them. The proposal should be presented in a professional and persuasive style, utilizing evidence-based practices and research to support its recommendations.</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mExecutive Summary:\u001b[0m\n",
      "\u001b[34mThe proposal aims to introduce a wellness program in our company that promotes the physical, mental, and emotional well-being of employees. Research has shown that such programs lead to increased job satisfaction, employee engagement, and productivity, and reduce healthcare costs. After conducting a needs assessment and analyzing data, we have identified three focus areas: physical activity, mental health, and nutrition. Based on the evidence from research and best practices, we have developed a comprehensive program that includes various interventions and strategies. The program will be implemented in three phases over one year, with a budget of $250,000. The outcome measures include the reduction of absenteeism, healthcare costs, and stress levels, and the improvement of employee engagement, morale, and productivity. The proposal also addresses potential barriers and suggests solutions to overcome them.\u001b[0m\n",
      "\u001b[34mObjectives:\u001b[0m\n",
      "\u001b[34mThe objectives of the employee wellness program are:\u001b[0m\n",
      "\u001b[34m• To promote the physical, mental, and emotional well-being of employees\u001b[0m\n",
      "\u001b[34m• To reduce absenteeism and presenteeism due to illness or stress\u001b[0m\n",
      "\u001b[34m• To improve employee engagement, morale, and productivity\u001b[0m\n",
      "\u001b[34m• To reduce healthcare costs and insurance premiums\u001b[0m\n",
      "\u001b[34m• To enhance the company's reputation as an employer of choice\u001b[0m\n",
      "\u001b[34m• To create a culture of health and wellness in the workplace\u001b[0m\n",
      "\u001b[34mStrategies and Interventions:\u001b[0m\n",
      "\u001b[34mPhysical Activity:\u001b[0m\n",
      "\u001b[34mThe physical activity component of the wellness program aims to encourage employees to engage in regular exercise and increase their physical activity levels. The following interventions will be implemented:\u001b[0m\n",
      "\u001b[34m• Provide on-site fitness classes, such as yoga, pilates, and Zumba, at least twice a week\u001b[0m\n",
      "\u001b[34m• Offer standing desks, treadmill workstations, and other ergonomic equipment to promote movement and reduce sedentary behavior\u001b[0m\n",
      "\u001b[34m• Organize walking challenges, such as step counting competitions, with prizes for the winners\u001b[0m\n",
      "\u001b[34m• Provide healthy snacks in the break room, such as fresh fruit, granola bars, and nuts, to fuel physical activity\u001b[0m\n",
      "\u001b[34mMental Health:\u001b[0m\n",
      "\u001b[34mThe mental health component of the wellness program aims to promote psychological well-being, reduce stress levels, and increase resilience. The following interventions will be implemented:\u001b[0m\n",
      "\u001b[34m• Provide access to counseling services, either in-person or via teletherapy, for employees who need support\u001b[0m\n",
      "\u001b[34m• Organize stress management workshops, such as meditation, mindfulness, and deep breathing exercises\u001b[0m\n",
      "\u001b[34m• Offer mental health first aid training to managers and supervisors to enable them to identify and support employees who may be struggling\u001b[0m\n",
      "\u001b[34m• Provide resources and information on mental health, such as self-help books, apps, and websites\u001b[0m\n",
      "\u001b[34mNutrition:\u001b[0m\n",
      "\u001b[34mThe nutrition component of the wellness program aims to promote healthy eating habits and reduce unhealthy behaviors, such as junk food consumption and overeating. The following interventions will be implemented:\u001b[0m\n",
      "\u001b[34m• Offer healthy food options in the cafeteria, such as salads, vegetable-based dishes, and lean protein sources\u001b[0m\n",
      "\u001b[34m• Promote healthy eating through educational workshops and seminars on topics such as meal planning, portion control, and mindful eating\u001b[0m\n",
      "\u001b[34m• Provide healthy snack options in vending machines, such as fruit, trail mix, and low-fat popcorn\u001b[0m\n",
      "\u001b[34m• Offer nutrition counseling and coaching services for employees who want to make dietary improvements\u001b[0m\n",
      "\u001b[34mTimelines:\u001b[0m\n",
      "\u001b[34mThe employee wellness program will be implemented in three phases over one year:\u001b[0m\n",
      "\u001b[34mPhase 1: Needs Assessment and Planning (Month 1-2)\u001b[0m\n",
      "\u001b[34m• Conduct an employee survey to gather data on health behaviors, preferences, and needs\u001b[0m\n",
      "\u001b[34m• Analyze data and identify focus areas and target populations\u001b[0m\n",
      "\u001b[34m• Develop the program components, interventions, and strategies\u001b[0m\n",
      "\u001b[34m• Establish a wellness committee to oversee the program\u001b[0m\n",
      "\u001b[34mPhase 2: Implementation (Month 3-9)\u001b[0m\n",
      "\u001b[34m• Roll out the program interventions and strategies in the three focus areas\u001b[0m\n",
      "\u001b[34m• Offer ongoing education and support to employees on wellness topics\u001b[0m\n",
      "\u001b[34m• Evaluate the program's effectiveness and make adjustments as needed\u001b[0m\n",
      "\u001b[34mPhase 3: Maintenance and Evaluation (Month 10-12)\u001b[0m\n",
      "\u001b[34m• Continue to offer the program interventions and monitor results\u001b[0m\n",
      "\u001b[34m• Collect and analyze program data to evaluate outcomes and make recommendations for future improvements\u001b[0m\n",
      "\u001b[34m• Celebrate successes and recognize employee achievements\u001b[0m\n",
      "\u001b[34mBudget Estimates:\u001b[0m\n",
      "\u001b[34mThe total budget for the employee wellness program is $250,000. The breakdown of expenses is as follows:\u001b[0m\n",
      "\u001b[34m• Fitness classes and equipment: $80,000\u001b[0m\n",
      "\u001b[34m• Counseling services and mental health first aid training: $60,000\u001b[0m\n",
      "\u001b[34m• Nutrition counseling and healthy food options: $40,000\u001b[0m\n",
      "\u001b[34m• Program materials and resources: $20,000\u001b[0m\n",
      "\u001b[34m• Program evaluation and data analysis: $30,000\u001b[0m\n",
      "\u001b[34m• Wellness committee and staff time: $20,000\u001b[0m\n",
      "\u001b[34mMetrics for Measuring Success:\u001b[0m\n",
      "\u001b[34mThe outcome measures for the program include:\u001b[0m\n",
      "\u001b[34m• Reduction of absenteeism and presenteeism due to illness or stress\u001b[0m\n",
      "\u001b[34m• Reduction of healthcare costs and insurance premiums\u001b[0m\n",
      "\u001b[34m• Improvement in employee engagement, morale, and productivity\u001b[0m\n",
      "\u001b[34m• Improvement in health behaviors, such as physical activity and healthy eating\u001b[0m\n",
      "\u001b[34m• Reduction of stress levels\u001b[0m\n",
      "\u001b[34mPotential Barriers and Solutions:\u001b[0m\n",
      "\u001b[34mPotential barriers to the success of the program include:\u001b[0m\n",
      "\u001b[34m• Lack of employee participation and motivation\u001b[0m\n",
      "\u001b[34m• Resistance from managers and supervisors\u001b[0m\n",
      "\u001b[34m• Budget constraints\u001b[0m\n",
      "\u001b[34mThe following solutions are recommended to overcome these barriers:\u001b[0m\n",
      "\u001b[34m• Use incentives, such as prizes and rewards,</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mThanks for generating such a comprehensive proposal for the employee wellness program. Could you please provide some more information on how the program will be evaluated and how we can measure its success?</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mSure! Evaluation and metrics are critical to determining if the employee wellness program is successful and aligns with its objectives. Here are some ways to measure the success of the program:\u001b[0m\n",
      "\u001b[34m1. Absenteeism and Presenteeism: The program's success can be measured by tracking the number of days of absenteeism and presenteeism due to stress or illness. Keep a record of the data for each quarter and compare it to previous years to get an accurate picture of the program's effectiveness.\u001b[0m\n",
      "\u001b[34m2. Health Behaviors: The program can be evaluated by monitoring changes in health behaviors such as physical activity levels, healthy eating, and stress management strategies. This can be done by conducting surveys and using activity trackers to monitor employee activity levels.\u001b[0m\n",
      "\u001b[34m3. Healthcare Costs and Insurance premiums: The healthcare costs and insurance premiums can be analyzed to compare the ROI of implementing the employee wellness program with those of other years, to see if the program results in reduced healthcare costs.\u001b[0m\n",
      "\u001b[34m4. Employee Engagement and Productivity: Employee satisfaction surveys can be conducted to evaluate their experiences of the program, and productivity can be measured by tracking the number of positive workdays.\u001b[0m\n",
      "\u001b[34m5. Compliance with Participation: The number of consumer engagement in the program should be also monitored to track adherence and compliance with the program.\u001b[0m\n",
      "\u001b[34m6. Specific Program Results: Specific program measures can be developed to evaluate each component of the wellness program. For instance, the number of employees participating in a stress management workshop can be evaluated, similarly, the percentage of employees who joined fitness classes every week, employees who made dietary changes, and other changes in wellness behavior can be tracked.\u001b[0m\n",
      "\u001b[34mOverall, the program's outcomes will be measured using several metrics to determine how successful it is in terms of reaching the objectives defined. An adequate evaluation plan is critical when implementing the program to ensure that interventions and strategies are relevant and effective.</s>\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:02 - INFO - __main__ - Sample 51 of the processed training set:\u001b[0m\n",
      "\u001b[34m<|system|>\u001b[0m\n",
      "\u001b[34m</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mWrite a 5 to 7-page memoir about the role parental pressure played in your eating disorder. Please include specific examples of times when you felt pressured to meet certain expectations related to your weight, appearance, or diet. Additionally, consider exploring the emotional impact of this pressure on your relationship with your parents and how it may have affected other areas of your life. Use a conversational tone and feel free to incorporate personal anecdotes and reflections as appropriate.</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mAs I sit down to write this memoir about my eating disorder, I can't help but feel a sense of vulnerability. Talking about my struggles with food and my body is not easy, especially when it comes to the role that my parents played in the development of my disorder. However, I know that sharing my story can help others who may be going through similar struggles.\u001b[0m\n",
      "\u001b[34mGrowing up, my parents placed a lot of importance on being thin and physically attractive. My mom was always dieting, counting calories and obsessing over her weight. She would make comments about her own body, saying things like \"I have to lose 10 pounds before my reunion\" or \"I can't believe how big my thighs look in these jeans.\" Her constant fixation on her appearance made me feel as though it was the most important thing in the world.\u001b[0m\n",
      "\u001b[34mWhen I was around 12, I started to gain weight. My mom would make comments about my body, saying things like \"you're getting a little chubby\" or \"maybe you should skip dessert tonight.\" My dad, too, would make comments about my weight, though he was less direct than my mom. He would suggest that I take up a sport or activity to \"stay in shape,\" or that I should \"watch what I eat\" to avoid gaining more weight.\u001b[0m\n",
      "\u001b[34mTheir comments affected me deeply. I started to feel as though my body was always under scrutiny, as though I wasn't good enough just the way I was. I started to feel ashamed of myself, and my confidence plummeted. Food became my refuge, my way of coping with the stress and anxiety that came with feeling like I was constantly failing.\u001b[0m\n",
      "\u001b[34mAt first, I would sneak treats and hide them in my room. I would hoard candy and chips, eating them when no one was around. But soon, my eating spiraled out of control. I would binge eat entire containers of ice cream, eat whole pizza pies by myself, or consume entire bags of chips in one sitting. I felt out of control, as though my hunger was a monster that I couldn't tame. I hated myself for how much I was eating and how little control I had over it.\u001b[0m\n",
      "\u001b[34mMy parents started to notice that I was gaining weight rapidly, and they became more vocal about it. My mom started to make more pointed comments about my size, saying things like \"are you sure you need that second helping?\" or \"maybe you should start watching your weight.\" My dad would comment on my eating habits, asking why I was always going back for more food or questioning why I wasn't losing weight despite my \"healthy\" diet.\u001b[0m\n",
      "\u001b[34mTheir comments made me feel even more ashamed and out of control. I started to feel like my body was the source of all my problems, that my weight was what defined me. I became more and more isolated, preferring to spend time alone rather than with my family. I stopped participating in activities I used to enjoy, like sports or music, because I felt like I didn't deserve to enjoy them since I was overweight.\u001b[0m\n",
      "\u001b[34mAs I got older, my eating disorder morphed into different forms. Sometimes I would restrict my eating, trying to be as skinny as possible. Other times I would purge, making myself throw up after meals or taking laxatives to get rid of the food I had eaten. My parents started to notice that something was wrong, but they didn't understand what was happening.\u001b[0m\n",
      "\u001b[34mWhen they asked me about my eating habits, I would lie and say that everything was fine. I didn't know how to explain to them that their comments and pressure had caused me to spiral into this pattern of self-destructive behavior. I didn't know how to ask for help, or how to tell them that I needed them to stop commenting on my body and my eating habits.\u001b[0m\n",
      "\u001b[34mIt took many years and a lot of therapy to undo the damage that my parents' pressure and comments had caused. But even now, years later, I still struggle with feeling like my body isn't good enough. I still have moments where I feel like I need to restrict my eating or purge after a meal. But I know now that these feelings aren't my fault and that I deserve to be happy and healthy no matter what my body looks like.\u001b[0m\n",
      "\u001b[34mLooking back, I wish my parents had known how much their comments hurt me, how they caused me to develop an unhealthy relationship with food and my body. I wish they had known that their love and acceptance was more important than any number on a scale or the size of my clothing. But I also know that they were doing the best they could with the tools they had, and that they loved me to the best of their ability.\u001b[0m\n",
      "\u001b[34mNow, I am working on healing and finding ways to love and accept my body. It's not easy, but it's worth it. I hope that by sharing my story, I can help others who may be struggling with similar issues. Remember, you are more than your body, and you deserve to be happy and healthy no matter what.</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mIt was very insightful and relatable. Could you add more information about how your eating disorder affected your mental health and daily life?</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mSure, I'd be happy to add more about how my eating disorder affected my mental health and daily life. As my eating disorder progressed, it began to take a toll on my mental health. I became increasingly anxious and depressed, feeling as though I was always on edge or in a low mood. I started to feel ashamed of myself, almost as though I was a failure because I couldn't control my eating habits. These negative emotions began to consume me, making it difficult to focus on anything else. I found myself obsessing about my weight and my body image, constantly comparing myself to others and feeling like I didn't measure up. This, in turn, made it harder to interact with people or participate in activities that I used to enjoy. For example, I started to become more withdrawn, spending more time alone and avoiding social situations. I didn't want to go out to eat with friends or go to parties because I was afraid of being judged or being tempted by food. This sense of isolation only made my mental health worse, as I became more and more stuck in my own negative thoughts. My daily life was also affected in more practical ways. My eating disorder made it difficult for me to maintain a healthy diet or exercise routine. I was either overeating or undereating, never finding a balance that worked for me. This also meant that I struggled with maintaining a healthy weight, which only perpetuated the negative feelings I had about my body. Additionally, my eating disorder affected my ability to concentrate and be productive. When I was in the midst of a binge or purge episode, I couldn't focus on anything else. I would procrastinate on schoolwork or sit at my desk for hours, unable to get anything done. This, again, made me feel like a failure and only added to my stress and anxiety. Overall, my eating disorder was a pervasive presence in my life, affecting everything from my mental health to my daily routines. It took a lot of time and effort to overcome, but I'm grateful to be in a better place now.</s>\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:02 - INFO - __main__ - Sample 563 of the processed training set:\u001b[0m\n",
      "\u001b[34m<|system|>\u001b[0m\n",
      "\u001b[34m</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mExplain the cultural significance of the symbolism behind the leaning Tower of Pisa in Italian art history, and provide examples of how it has been represented in paintings, literature and other media throughout the centuries.</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mThe Leaning Tower of Pisa has been a symbol of Italian art and culture for centuries. The significance of this leaning tower lies in its unique architectural design, which has come to represent the ingenuity, innovation, and creative spirit of the Italian people.\u001b[0m\n",
      "\u001b[34mIn Italian art history, the tower has been a prominent subject in paintings and other forms of media, often serving as a metaphor for the human condition. Artists have used the tower to convey a range of themes, including the fragility of human existence, the passing of time, and the struggles that people face in life.\u001b[0m\n",
      "\u001b[34mOne of the most famous examples of the Leaning Tower in art is the painting by Giovanni Paolo Panini, \"Interior of a Picture Gallery with the Collection of Cardinal Silvio Valenti Gonzaga.\" This painting depicts the tower in the background, leaning dramatically over the cityscape. The painting is significant because it demonstrates the Leaning Tower's association with the past and the idea of Italy as a nation with a rich cultural heritage.\u001b[0m\n",
      "\u001b[34mIn literature, the Leaning Tower of Pisa has been referenced countless times over the years. In Dante Alighieri's famous work, \"The Divine Comedy,\" the tower is mentioned as a symbol of sin and evil, and as a warning against the dangers of pride and arrogance.\u001b[0m\n",
      "\u001b[34mOverall, the cultural significance of the Leaning Tower of Pisa lies in its metaphorical power as a symbol of human struggle and achievement. It has been represented in countless works of art and literature throughout the centuries, serving as a reminder of the importance of innovation, creativity, and perseverance in the face of adversity.</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mThat's a really interesting explanation! Could you give me some more examples of how the Leaning Tower of Pisa has been represented in literature besides Dante's \"The Divine Comedy\"? And also, are there any other famous paintings or artworks that feature the Tower?</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mYes, of course! The Leaning Tower of Pisa has been referenced in literature beyond Dante's \"The Divine Comedy.\" For example, in William Shakespeare's play \"Twelfth Night,\" the character Sir Andrew Aguecheek compares his dancing to the leaning tower in order to mock himself:\u001b[0m\n",
      "\u001b[34m\"I can see no rhyme in the reason why I should dance \u001b[0m\n",
      "\u001b[34mbarefoot to the tower like a peasant.\"\u001b[0m\n",
      "\u001b[34mAdditionally, the tower has been described in works by other authors such as Giuseppe Giusti and Giovanni Boccaccio.\u001b[0m\n",
      "\u001b[34mIn terms of paintings and artworks, the Tower has been featured in many works throughout the centuries. One of the most famous is the \"View of Pisa\" by Joachim Patinir, which was painted around 1520. The painting shows the Tower prominently, leaning in the background over the city, and it is believed to be among the first depictions of the Tower in art.\u001b[0m\n",
      "\u001b[34mAnother famous work is \"The Leaning Tower of Pisa\" by Eugène Delacroix. This painting depicts the Tower from a dramatic, low angle and captures the sense of precariousness and instability that is associated with the structure. \u001b[0m\n",
      "\u001b[34mIn contemporary art, the Tower has also been used as a subject, such as in David Mach's sculpture \"Big Heids in Pisa\" which features giant matchstick figures leaning against the Tower.\u001b[0m\n",
      "\u001b[34mOverall, the Leaning Tower of Pisa has been a popular subject in literature and visual art throughout the years, and it continues to captivate and inspire people from all over the world.</s>\u001b[0m\n",
      "\u001b[34m<|user|>\u001b[0m\n",
      "\u001b[34mCould you also tell me some fun facts about the Leaning Tower of Pisa? It would be interesting to learn more about it beyond its cultural significance.</s>\u001b[0m\n",
      "\u001b[34m<|assistant|>\u001b[0m\n",
      "\u001b[34mSure, here are some interesting facts about the Leaning Tower of Pisa:\u001b[0m\n",
      "\u001b[34m1. The Tower started leaning shortly after construction began in 1173. It is believed that the ground was not stable enough to support the weight of the structure and caused it to sink on one side. 2. The Tower is not alone. It is actually one of four buildings that make up the Piazza dei Miracoli (Square of Miracles) complex in Pisa, Italy. The others include the Cathedral, the Baptistery, and the Camposanto Monumentale. 3. The Tower is not perfectly straight, but rather, it has a slight curve. The architects corrected the angle of the Tower during construction to try to compensate for its leaning position, but this actually caused the Tower to have a bit of a curve in its shape. 4. The Tower stands at 56 meters (183.3 feet) tall and has a weight of approximately 14,500 tons. 5. The construction of the Tower took over 200 years to complete. It was originally supposed to be completed in just two decades, but delays and interruptions caused the project to stretch out over centuries. 6. The Tower was almost destroyed during World War II when the German army occupied the area. Luckily, an American Army Sergeant named Leon Weckstein recognized the cultural and historical significance of the building and made sure that Allied forces did not bomb it. 7. In 2008, the Tower was fitted with a set of 11 counterweights to help stabilize it and prevent further leaning. These weights were installed under the base of the Tower and have been successful in reducing the amount of leaning. 8. The Leaning Tower of Pisa has been featured in many pop culture references, such as in video games, movies, and television shows. Some examples include the video game \"Assassin's Creed II,\" the movie \"The Italian Job,\" and the TV show \"The Simpsons.\" I hope you found those facts interesting!</s>\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.5+cuda12.1\u001b[0m\n",
      "\u001b[35malgo-2:201:256 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:201:256 [0] nccl_net_ofi_init:1239 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:03 - INFO - __main__ - *** Load pretrained model ***\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:03 - INFO - __main__ - *** Model loaded! ***\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:145: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:715] 2024-05-09 17:27:03,933 >> loading configuration file /opt/ml/input/data/model/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:715] 2024-05-09 17:27:03,933 >> loading configuration file /opt/ml/input/data/model/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:777] 2024-05-09 17:27:03,934 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:777] 2024-05-09 17:27:03,934 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3118] 2024-05-09 17:27:03,937 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3118] 2024-05-09 17:27:03,937 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3184] 2024-05-09 17:27:03,938 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3184] 2024-05-09 17:27:03,938 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:1222] 2024-05-09 17:27:03,938 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:1222] 2024-05-09 17:27:03,938 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:791] 2024-05-09 17:27:03,939 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:791] 2024-05-09 17:27:03,939 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34malgo-1:202:258 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:202:258 [0] nccl_net_ofi_init:1239 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:03 - INFO - __main__ - *** Load pretrained model ***\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:03 - INFO - __main__ - *** Model loaded! ***\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:145: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:715] 2024-05-09 17:27:03,933 >> loading configuration file /opt/ml/input/data/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:715] 2024-05-09 17:27:03,933 >> loading configuration file /opt/ml/input/data/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:777] 2024-05-09 17:27:03,934 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:777] 2024-05-09 17:27:03,934 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3118] 2024-05-09 17:27:03,937 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3118] 2024-05-09 17:27:03,937 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3184] 2024-05-09 17:27:03,937 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3184] 2024-05-09 17:27:03,937 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1222] 2024-05-09 17:27:03,937 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1222] 2024-05-09 17:27:03,937 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:791] 2024-05-09 17:27:03,939 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:791] 2024-05-09 17:27:03,939 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3257] 2024-05-09 17:27:04,594 >> Detected 4-bit loading: activating 4-bit loading for this model\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3257] 2024-05-09 17:27:04,594 >> Detected 4-bit loading: activating 4-bit loading for this model\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/alignment-handbook/scripts/run_sft.py\", line 203, in <module>\u001b[0m\n",
      "\u001b[35mmain()\n",
      "  File \"/opt/ml/code/alignment-handbook/scripts/run_sft.py\", line 139, in main\u001b[0m\n",
      "\u001b[35mtrainer = SFTTrainer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 149, in __init__\u001b[0m\n",
      "\u001b[35mmodel = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\u001b[0m\n",
      "\u001b[35mreturn model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3480, in from_pretrained\u001b[0m\n",
      "\u001b[35m) = cls._load_pretrained_model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3856, in _load_pretrained_model\u001b[0m\n",
      "\u001b[35mstate_dict = load_state_dict(shard_file)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 467, in load_state_dict\u001b[0m\n",
      "\u001b[35mwith safe_open(checkpoint_file, framework=\"pt\") as f:\u001b[0m\n",
      "\u001b[35mFileNotFoundError: No such file or directory: \"/opt/ml/input/data/model/model-00001-of-00002.safetensors\"\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3257] 2024-05-09 17:27:04,587 >> Detected 4-bit loading: activating 4-bit loading for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3257] 2024-05-09 17:27:04,587 >> Detected 4-bit loading: activating 4-bit loading for this model\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/alignment-handbook/scripts/run_sft.py\", line 203, in <module>\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"/opt/ml/code/alignment-handbook/scripts/run_sft.py\", line 139, in main\u001b[0m\n",
      "\u001b[34mtrainer = SFTTrainer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 149, in __init__\u001b[0m\n",
      "\u001b[34mmodel = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\u001b[0m\n",
      "\u001b[34mreturn model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3480, in from_pretrained\u001b[0m\n",
      "\u001b[34m) = cls._load_pretrained_model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3856, in _load_pretrained_model\u001b[0m\n",
      "\u001b[34mstate_dict = load_state_dict(shard_file)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 467, in load_state_dict\u001b[0m\n",
      "\u001b[34mwith safe_open(checkpoint_file, framework=\"pt\") as f:\u001b[0m\n",
      "\u001b[34mFileNotFoundError: No such file or directory: \"/opt/ml/input/data/model/model-00001-of-00002.safetensors\"\u001b[0m\n",
      "\u001b[35m[2024-05-09 17:27:07,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 201) of binary: /opt/conda/bin/python\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\u001b[0m\n",
      "\u001b[35msys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\u001b[0m\n",
      "\u001b[35mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\u001b[0m\n",
      "\u001b[35mrun(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\u001b[0m\n",
      "\u001b[35melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\u001b[0m\n",
      "\u001b[35mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\u001b[0m\n",
      "\u001b[35mraise ChildFailedError(\u001b[0m\n",
      "\u001b[35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \u001b[0m\n",
      "\u001b[35m============================================================\u001b[0m\n",
      "\u001b[35malignment-handbook/scripts/run_sft.py FAILED\u001b[0m\n",
      "\u001b[35m------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mFailures:\n",
      "  <NO_OTHER_FAILURES>\u001b[0m\n",
      "\u001b[35m------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[35m[0]:\n",
      "  time      : 2024-05-09_17:27:07\n",
      "  host      : algo-2\n",
      "  rank      : 1 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 201)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[35m============================================================\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:07,360 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:07,360 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:07,361 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:07,361 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mExitCode 1\u001b[0m\n",
      "\u001b[35mErrorMessage \"FileNotFoundError: No such file or directory: \"/opt/ml/input/data/model/model-00001-of-00002.safetensors\"\n",
      " [2024-05-09 17:27:07,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 201) of binary: /opt/conda/bin/python\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      " sys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors.ChildFailedError\n",
      " ============================================================\n",
      " alignment-handbook/scripts/run_sft.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " <NO_OTHER_FAILURES>\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " time      : 2024-05-09_17:27:07\n",
      " host      : algo-2\n",
      " rank      : 1 (local_rank: 0)\n",
      " exitcode  : 1 (pid: 201)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\"\u001b[0m\n",
      "\u001b[35mCommand \"torchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 1 alignment-handbook/scripts/run_sft.py --recipe config_sft_lora.yaml\"\u001b[0m\n",
      "\u001b[35m2024-05-09 17:27:07,361 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\u001b[34m[2024-05-09 17:27:07,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 202) of binary: /opt/conda/bin/python\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\u001b[0m\n",
      "\u001b[34msys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\u001b[0m\n",
      "\u001b[34mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\u001b[0m\n",
      "\u001b[34mrun(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\u001b[0m\n",
      "\u001b[34melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\u001b[0m\n",
      "\u001b[34mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\u001b[0m\n",
      "\u001b[34mraise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors.\u001b[0m\n",
      "\u001b[34mChildFailedError: \u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34malignment-handbook/scripts/run_sft.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\n",
      "  <NO_OTHER_FAILURES>\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\n",
      "  time      : 2024-05-09_17:27:07\n",
      "  host      : algo-1\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 202)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:07,349 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:07,349 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:07,349 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:07,349 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"FileNotFoundError: No such file or directory: \"/opt/ml/input/data/model/model-00001-of-00002.safetensors\"\n",
      " [2024-05-09 17:27:07,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 202) of binary: /opt/conda/bin/python\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      " sys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors.\n",
      " ChildFailedError\n",
      " ============================================================\n",
      " alignment-handbook/scripts/run_sft.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " <NO_OTHER_FAILURES>\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " time      : 2024-05-09_17:27:07\n",
      " host      : algo-1\n",
      " rank      : 0 (local_rank: 0)\n",
      " exitcode  : 1 (pid: 202)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\"\u001b[0m\n",
      "\u001b[34mCommand \"torchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 0 alignment-handbook/scripts/run_sft.py --recipe config_sft_lora.yaml\"\u001b[0m\n",
      "\u001b[34m2024-05-09 17:27:07,349 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-05-09 17:27:27 Uploading - Uploading generated training model\n",
      "2024-05-09 17:27:27 Failed - Instances not retained as a result of warmpool resource limits being exceeded\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job mistral7b-sft-2024-05-09-17-20-42-888: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"FileNotFoundError: No such file or directory: \"/opt/ml/input/data/model/model-00001-of-00002.safetensors\"\n [2024-05-09 17:27:07,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 202) of binary: /opt/conda/bin/python\n Traceback (most recent call last)\n File \"/opt/conda/bin/torchrun\", line 33, in <module>\n sys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n return f(*args, **kwargs)\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\n run(args)\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n elastic_launch(\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n return launch_agent(self._config, self._entrypoint,",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Invoking the fit method on the estimator starts the training job\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# data will be copied into training cluster based on the dictionary keys specified here\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The contents of the s3_model_location will be copied into the /opt/ml/input/data/model directory\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# The contents of the s3_data will be copied into the /opt/ml/input/data/train directory\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msft_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_model_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms3_data\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/sft_split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1346\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2703\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5797\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5778\u001b[0m \n\u001b[1;32m   5779\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5797\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:8026\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8023\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   8025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 8026\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   8028\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:8079\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8075\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8076\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8077\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8078\u001b[0m     )\n\u001b[0;32m-> 8079\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8080\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8081\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8082\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8083\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job mistral7b-sft-2024-05-09-17-20-42-888: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"FileNotFoundError: No such file or directory: \"/opt/ml/input/data/model/model-00001-of-00002.safetensors\"\n [2024-05-09 17:27:07,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 202) of binary: /opt/conda/bin/python\n Traceback (most recent call last)\n File \"/opt/conda/bin/torchrun\", line 33, in <module>\n sys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n return f(*args, **kwargs)\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\n run(args)\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n elastic_launch(\n File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n return launch_agent(self._config, self._entrypoint,"
     ]
    }
   ],
   "source": [
    "# Invoking the fit method on the estimator starts the training job\n",
    "# data will be copied into training cluster based on the dictionary keys specified here\n",
    "# The contents of the s3_model_location will be copied into the /opt/ml/input/data/model directory\n",
    "# The contents of the s3_data will be copied into the /opt/ml/input/data/train directory\n",
    "sft_estimator.fit({\"model\": s3_model_location, \"train\": f\"{s3_data}/sft_split\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker Training Job for Direct Preference Optimization\n",
    "Once we have the fine-tuned model, we can further fine-tune it using Direct Preference Optimization to better align with our preferences and improve the model's outputs. The code process is similar to the supervised fine-tuning job. Except now we will use the `alignment-handbook/scripts/run_dpo.py` script and also provide our fine-tuned model as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/fine-tuning-mistral/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "job_name = f\"mistral7b-dpo\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"recipe\": \"config_dpo_lora.yaml\",\n",
    "}\n",
    "\n",
    "dpo_estimator = PyTorch(\n",
    "    base_job_name=job_name,\n",
    "    source_dir = \"src\",\n",
    "    entry_point=\"alignment-handbook/scripts/run_dpo.py\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2, \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    disable_output_compression = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the location of the fine tuned model from the sft_estimator\n",
    "sft_model_location = sft_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LoRA was used for fine-tuning, or SFT model will only contain the LoRA adapter. Therefore we also provide the base model as another input into the `.fit` call below. The training script will automatically merge the base model with the LoRA adapter and then proceed with the DPO fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_estimator.fit(\n",
    "    {\n",
    "        \"model\": s3_model_location,       # base Mistral 7B model \n",
    "        \"sft_model\": sft_model_location,  # fine-tuned model from the previous step\n",
    "        \"train\": f\"{s3_data}/dpo_split\",  # preference training data\n",
    "    }\n",
    ")\n",
    "dpo_model_location = dpo_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Run these cells to remove the data and model artifacts from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive $s3_model_location \n",
    "!aws s3 rm --recursive $sft_model_location\n",
    "!aws s3 rm --recursive $dpo_model_location\n",
    "!aws s3 rm --recursive $s3_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
